{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from itertools import product\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Mount Google Drive (skip if already mounted)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Add custom KAN paths\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/packages/efficient_kan')\n",
    "sys.path.append('/content/drive/MyDrive/packages/fastkan')\n",
    "\n",
    "# Import KAN and FastKAN classes\n",
    "from kan import *\n",
    "from fastkan import *\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\"\"\"\nKolmogorov-Arnold Networks (KAN) for Crayfish Sex Classification\n================================================================\n\nOriginal Paper Authors:\n    Yasin Atilkan¹, Berk Kirik², Eren Tuna Acikbas³, Fatih Ekinci⁴,\n    Koray Acici¹'⁴, Tunc Asuroglu⁵'⁶*, Recep Benzer⁷,\n    Mehmet Serdar Guzel⁴'⁸, Semra Benzer⁹\n\nAffiliations:\n    ¹ Department of Artificial Intelligence and Data Engineering, Ankara University, Turkey\n    ² Department of Biomedical Engineering, Ankara University, Turkey\n    ³ Department of Petroleum and Natural Gas Engineering, Middle East Technical University, Turkey\n    ⁴ Institute of Artificial Intelligence, Ankara University, Turkey\n    ⁵ Faculty of Medicine and Health Technology, Tampere University, Finland\n    ⁶ VTT Technical Research Centre of Finland, Finland\n    ⁷ Department of Management Information System, Ankara Medipol University, Turkey\n    ⁸ Department of Computer Engineering, Ankara University, Turkey\n    ⁹ Department of Science Education, Gazi University, Turkey\n    * Corresponding author\n\nImplementation follows the methodology from:\n\"Enhancing Crayfish Sex Identification with Kolmogorov-Arnold Networks\nand Stacked Autoencoders\" (Atilkan et al., 2024)\n\nKey Features:\n- Grid search for optimal hyperparameter selection\n- 10-Fold Cross-Validation with stratified sampling\n- Per-fold data standardization (prevents data leakage)\n- Early stopping mechanism for overfitting mitigation\n- Comprehensive evaluation on both test folds and full dataset\n- Statistical analysis and visualization\n- Best model identification and storage\n\nImplementation Date: 2024\nLicense: MIT\n\"\"\"\n\n\nimport os\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Tuple, Any\nimport warnings\nfrom pathlib import Path\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader as TorchDataLoader, TensorDataset\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    confusion_matrix, accuracy_score, precision_score,\n    recall_score, f1_score, matthews_corrcoef, classification_report\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import product\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore')\n\n\nclass Config:\n    \"\"\"Configuration class for KAN model training and evaluation.\"\"\"\n\n    DATA_PATH = '/content/drive/MyDrive/crawfish_data/kerevit_knn_final.xlsx'\n    OUTPUT_PATH = '/content/drive/MyDrive/crawfish_results/ml_results/kan_grid_search'\n\n    FEATURES = ['W', 'KB', 'KE', 'AB', 'AE', 'K_Sag', 'K_Sol', 'U_Sag', 'U_Sol', 'KE_Sag', 'KE_Sol']\n    TARGET = 'CINSIYET'\n\n    GRID_SEARCH_PARAMS = {\n        'layers_hidden': [\n            [11, 256, 128, 64, 32, 1],\n            [11, 128, 64, 32, 1],\n            [11, 32, 1],\n            [11, 128, 64, 32, 16, 1]\n        ],\n        'grid_size': [8, 7, 6, 5, 4],\n        'spline_order': [5, 4, 3],\n        'scale_base': [1.0, 2.0, 3.0],\n        'scale_spline': [1.0, 2.0],\n        'batch_size': [128, 64, 32],\n        'optimizer': ['Adam', 'SGD']\n    }\n\n    LEARNING_RATE = 0.0005\n    NUM_EPOCHS = 50\n    N_FOLDS = 10\n    RANDOM_STATE = 42\n\n    EARLY_STOPPING_PATIENCE = 10\n    EARLY_STOPPING_DELTA = 0.0\n\n    FIG_SIZE = (10, 8)\n    DPI = 300\n    CLASS_NAMES = ['Female (D)', 'Male (E)']\n\n    @classmethod\n    def display_config(cls, logger: logging.Logger) -> None:\n        \"\"\"Display current configuration.\"\"\"\n        logger.info(\"Configuration Summary:\")\n        logger.info(f\"  Total hyperparameter combinations: {cls._get_total_combinations()}\")\n        logger.info(f\"  Learning rate (fixed): {cls.LEARNING_RATE}\")\n        logger.info(f\"  Maximum epochs: {cls.NUM_EPOCHS}\")\n        logger.info(f\"  Cross-validation folds: {cls.N_FOLDS}\")\n        logger.info(f\"  Early stopping patience: {cls.EARLY_STOPPING_PATIENCE} epochs\")\n        logger.info(f\"  Early stopping delta: {cls.EARLY_STOPPING_DELTA}\")\n\n    @classmethod\n    def _get_total_combinations(cls) -> int:\n        \"\"\"Calculate total number of hyperparameter combinations.\"\"\"\n        return int(np.prod([len(v) for v in cls.GRID_SEARCH_PARAMS.values()]))\n\n\ndef setup_logging(output_path: str) -> logging.Logger:\n    \"\"\"Setup comprehensive logging system with file and console handlers.\"\"\"\n    Path(output_path).mkdir(parents=True, exist_ok=True)\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(output_path, f'kan_training_{timestamp}.log')\n\n    logger = logging.getLogger('KAN_Training')\n    logger.setLevel(logging.DEBUG)\n    logger.handlers = []\n    logger.propagate = False\n\n    file_handler = logging.FileHandler(log_file, encoding='utf-8')\n    file_handler.setLevel(logging.DEBUG)\n    file_formatter = logging.Formatter(\n        '%(asctime)s | %(levelname)-8s | %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n    file_handler.setFormatter(file_formatter)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.INFO)\n    console_formatter = logging.Formatter('%(message)s')\n    console_handler.setFormatter(console_formatter)\n\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n\n    logger.info(\"=\" * 80)\n    logger.info(\"KAN Training for Crayfish Sex Classification\")\n    logger.info(\"Based on: Atilkan et al. (2024)\")\n    logger.info(\"=\" * 80)\n    logger.info(f\"Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    logger.info(f\"Log file: {log_file}\")\n    logger.info(\"=\" * 80)\n\n    return logger\n\n\nclass EarlyStopping:\n    \"\"\"Early stopping mechanism to prevent overfitting.\"\"\"\n\n    def __init__(self, patience: int = 10, verbose: bool = False, delta: float = 0.0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.delta = delta\n        self.best_model_state = None\n\n    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n        \"\"\"Check if early stopping criteria is met.\"\"\"\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.best_model_state = model.state_dict()\n        elif val_loss > self.best_loss - self.delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n                model.load_state_dict(self.best_model_state)\n        else:\n            self.best_loss = val_loss\n            self.best_model_state = model.state_dict()\n            self.counter = 0\n\n    def reset(self) -> None:\n        \"\"\"Reset early stopping state.\"\"\"\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.best_model_state = None\n\n\ndef calculate_specificity(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculate specificity (true negative rate).\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    if cm.size == 1:\n        return 1.0\n    tn, fp, fn, tp = cm.ravel()\n    return tn / (tn + fp) if (tn + fp) > 0 else 0.0\n\n\ndef calculate_all_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n    \"\"\"Calculate all evaluation metrics.\"\"\"\n    return {\n        'Accuracy': accuracy_score(y_true, y_pred),\n        'Precision': precision_score(y_true, y_pred, zero_division=0),\n        'Recall': recall_score(y_true, y_pred, zero_division=0),\n        'Specificity': calculate_specificity(y_true, y_pred),\n        'F1-Score': f1_score(y_true, y_pred, zero_division=0),\n        'MCC': matthews_corrcoef(y_true, y_pred)\n    }\n\n\ndef save_confusion_matrix(\n    cm: np.ndarray,\n    filename: str,\n    class_names: List[str],\n    title: str = 'Confusion Matrix'\n) -> None:\n    \"\"\"Save confusion matrix as high-quality PNG image.\"\"\"\n    plt.figure(figsize=Config.FIG_SIZE)\n\n    sns.heatmap(\n        cm, annot=True, fmt='d', cmap='Blues',\n        xticklabels=class_names, yticklabels=class_names,\n        cbar=True, square=True, linewidths=1, linecolor='gray'\n    )\n\n    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n    plt.ylabel('True Label', fontsize=14, fontweight='bold')\n    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(filename, dpi=Config.DPI, bbox_inches='tight')\n    plt.close()\n\n\ndef print_section_header(title: str, logger: logging.Logger, level: int = 1) -> None:\n    \"\"\"Print professional section header.\"\"\"\n    logger.info(\"\")\n\n    if level == 1:\n        logger.info(\"=\" * 80)\n        logger.info(title)\n        logger.info(\"=\" * 80)\n    else:\n        logger.info(\"-\" * 80)\n        logger.info(title)\n        logger.info(\"-\" * 80)\n\n\ndef format_metrics_table(metrics: Dict[str, float]) -> str:\n    \"\"\"Format metrics dictionary as readable table.\"\"\"\n    lines = []\n    lines.append(\"-\" * 40)\n    for metric, value in metrics.items():\n        lines.append(f\"  {metric:<20} {value:>8.4f}\")\n    lines.append(\"-\" * 40)\n    return \"\\n\".join(lines)\n\n\nclass DataManager:\n    \"\"\"Handle data loading and preprocessing operations.\"\"\"\n\n    def __init__(self, config: Config, logger: logging.Logger):\n        self.config = config\n        self.logger = logger\n\n    def load_data(self) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n        \"\"\"Load dataset from Excel file.\"\"\"\n        self.logger.info(f\"Loading data: {self.config.DATA_PATH}\")\n\n        try:\n            df = pd.read_excel(self.config.DATA_PATH)\n            self.logger.info(f\"Successfully loaded {len(df)} samples\")\n        except Exception as e:\n            self.logger.error(f\"Failed to load data: {str(e)}\")\n            raise\n\n        X = df[self.config.FEATURES].copy()\n        y = df[self.config.TARGET].copy()\n        sira = df['Sira'].copy() if 'Sira' in df.columns else pd.Series(range(len(df)), name='Sira')\n\n        self._log_dataset_info(X, y)\n        return X, y, sira\n\n    def _log_dataset_info(self, X: pd.DataFrame, y: pd.Series) -> None:\n        \"\"\"Log dataset statistics.\"\"\"\n        self.logger.info(\"\")\n        self.logger.info(\"Dataset Statistics:\")\n        self.logger.info(f\"  Total samples: {len(X)}\")\n        self.logger.info(f\"  Number of features: {len(self.config.FEATURES)}\")\n\n        class_dist = y.value_counts()\n        self.logger.info(\"  Class distribution:\")\n        for cls, count in class_dist.items():\n            percentage = count / len(y) * 100\n            self.logger.info(f\"    Class {cls}: {count:3d} samples ({percentage:5.1f}%)\")\n\n\nclass KANTrainer:\n    \"\"\"KAN model training with comprehensive grid search.\"\"\"\n\n    def __init__(self, config: Config, logger: logging.Logger):\n        self.config = config\n        self.logger = logger\n\n    def train_single_fold(\n        self,\n        model: torch.nn.Module,\n        train_loader: TorchDataLoader,\n        val_loader: TorchDataLoader,\n        optimizer: torch.optim.Optimizer,\n        criterion: torch.nn.Module,\n        fold: int,\n        verbose: bool = False\n    ) -> Dict[str, Any]:\n        \"\"\"Train model for a single fold.\"\"\"\n        early_stopping = EarlyStopping(\n            patience=self.config.EARLY_STOPPING_PATIENCE,\n            verbose=verbose,\n            delta=self.config.EARLY_STOPPING_DELTA\n        )\n\n        history = {\n            'train_loss': [],\n            'val_loss': [],\n            'epochs_trained': 0,\n            'early_stopped': False\n        }\n\n        for epoch in range(self.config.NUM_EPOCHS):\n            model.train()\n            train_loss = 0.0\n            for X_batch, y_batch in train_loader:\n                optimizer.zero_grad()\n                outputs = model(X_batch).squeeze()\n                loss = criterion(outputs, y_batch)\n                loss.backward()\n                optimizer.step()\n                train_loss += loss.item()\n\n            avg_train_loss = train_loss / len(train_loader)\n            history['train_loss'].append(avg_train_loss)\n\n            model.eval()\n            val_loss = 0.0\n            with torch.no_grad():\n                for X_val, y_val in val_loader:\n                    val_outputs = model(X_val).squeeze()\n                    loss = criterion(val_outputs, y_val)\n                    val_loss += loss.item()\n\n            avg_val_loss = val_loss / len(val_loader)\n            history['val_loss'].append(avg_val_loss)\n\n            early_stopping(avg_val_loss, model)\n            if early_stopping.early_stop:\n                history['epochs_trained'] = epoch + 1\n                history['early_stopped'] = True\n                break\n        else:\n            history['epochs_trained'] = self.config.NUM_EPOCHS\n\n        return history\n\n    def evaluate_model(\n        self,\n        model: torch.nn.Module,\n        X_tensor: torch.Tensor,\n        y_tensor: torch.Tensor\n    ) -> Tuple[np.ndarray, np.ndarray, Dict[str, float]]:\n        \"\"\"Evaluate model and calculate metrics.\"\"\"\n        model.eval()\n        with torch.no_grad():\n            outputs = model(X_tensor).squeeze()\n            probabilities = torch.sigmoid(outputs).cpu().numpy()\n            predictions = (probabilities > 0.5).astype(int)\n            actuals = y_tensor.cpu().numpy()\n\n        metrics = calculate_all_metrics(actuals, predictions)\n        return predictions, probabilities, metrics\n\n    def grid_search(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        sira: pd.Series\n    ) -> Dict[str, Any]:\n        \"\"\"Perform grid search for optimal hyperparameters.\"\"\"\n        print_section_header(\"HYPERPARAMETER OPTIMIZATION VIA GRID SEARCH\", self.logger)\n\n        y_numeric = y.replace({'D': 0, 'E': 1})\n\n        param_combinations = list(product(\n            self.config.GRID_SEARCH_PARAMS['layers_hidden'],\n            self.config.GRID_SEARCH_PARAMS['grid_size'],\n            self.config.GRID_SEARCH_PARAMS['spline_order'],\n            self.config.GRID_SEARCH_PARAMS['scale_base'],\n            self.config.GRID_SEARCH_PARAMS['scale_spline'],\n            self.config.GRID_SEARCH_PARAMS['batch_size'],\n            self.config.GRID_SEARCH_PARAMS['optimizer']\n        ))\n\n        total_configs = len(param_combinations)\n        self.logger.info(f\"Total hyperparameter configurations: {total_configs}\")\n        self.logger.info(f\"Cross-validation folds per configuration: {self.config.N_FOLDS}\")\n        self.logger.info(f\"Total training iterations: {total_configs * self.config.N_FOLDS}\")\n        self.logger.info(\"\")\n\n        kf = KFold(n_splits=self.config.N_FOLDS, shuffle=False)\n\n        grid_results = []\n        best_accuracy = 0.0\n        best_params = None\n        best_config_id = None\n\n        start_time = datetime.now()\n\n        pbar = tqdm(\n            enumerate(param_combinations, 1),\n            total=total_configs,\n            desc=\"Grid Search Progress\",\n            ncols=120,\n            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n        )\n\n        for config_id, (layers_hidden, grid_size, spline_order, scale_base,\n                       scale_spline, batch_size, optimizer_name) in pbar:\n\n            self.logger.debug(\"\")\n            self.logger.debug(f\"Configuration ID: {config_id}/{total_configs}\")\n            self.logger.debug(f\"  Architecture: {layers_hidden}\")\n            self.logger.debug(f\"  Grid size: {grid_size}, Spline order: {spline_order}\")\n            self.logger.debug(f\"  Scale base: {scale_base}, Scale spline: {scale_spline}\")\n            self.logger.debug(f\"  Batch size: {batch_size}, Optimizer: {optimizer_name}\")\n\n            fold_accuracies = []\n\n            for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n                X_train = X.iloc[train_idx].copy()\n                X_test = X.iloc[test_idx].copy()\n                y_train = y_numeric.iloc[train_idx].copy()\n                y_test = y_numeric.iloc[test_idx].copy()\n\n                scaler = StandardScaler()\n                X_train_scaled = scaler.fit_transform(X_train)\n                X_test_scaled = scaler.transform(X_test)\n\n                X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n                y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n                X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n                y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n\n                train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n                train_loader = TorchDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n                val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n                val_loader = TorchDataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n                try:\n                    model = KAN(\n                        layers_hidden=list(layers_hidden),\n                        grid_size=grid_size,\n                        spline_order=spline_order,\n                        scale_base=scale_base,\n                        scale_spline=scale_spline\n                    )\n                except NameError:\n                    self.logger.error(\"KAN class not found. Please ensure KAN is properly imported.\")\n                    raise\n\n                if optimizer_name == 'Adam':\n                    optimizer = optim.Adam(model.parameters(), lr=self.config.LEARNING_RATE)\n                else:\n                    optimizer = optim.SGD(model.parameters(), lr=self.config.LEARNING_RATE, momentum=0.9)\n\n                criterion = torch.nn.BCEWithLogitsLoss()\n\n                history = self.train_single_fold(\n                    model=model,\n                    train_loader=train_loader,\n                    val_loader=val_loader,\n                    optimizer=optimizer,\n                    criterion=criterion,\n                    fold=fold,\n                    verbose=False\n                )\n\n                _, _, metrics = self.evaluate_model(model, X_test_tensor, y_test_tensor)\n                fold_accuracies.append(metrics['Accuracy'])\n\n                self.logger.debug(f\"  Fold {fold:2d}: Accuracy = {metrics['Accuracy']:.4f}\")\n\n            mean_accuracy = np.mean(fold_accuracies)\n            std_accuracy = np.std(fold_accuracies)\n            min_accuracy = np.min(fold_accuracies)\n            max_accuracy = np.max(fold_accuracies)\n\n            grid_results.append({\n                'configuration_id': config_id,\n                'architecture': str(layers_hidden),\n                'grid_size': grid_size,\n                'spline_order': spline_order,\n                'scale_base': scale_base,\n                'scale_spline': scale_spline,\n                'batch_size': batch_size,\n                'optimizer': optimizer_name,\n                'mean_accuracy': mean_accuracy,\n                'std_accuracy': std_accuracy,\n                'min_accuracy': min_accuracy,\n                'max_accuracy': max_accuracy\n            })\n\n            if mean_accuracy > best_accuracy:\n                best_accuracy = mean_accuracy\n                best_config_id = config_id\n                best_params = {\n                    'layers_hidden': list(layers_hidden),\n                    'grid_size': grid_size,\n                    'spline_order': spline_order,\n                    'scale_base': scale_base,\n                    'scale_spline': scale_spline,\n                    'batch_size': batch_size,\n                    'optimizer': optimizer_name\n                }\n\n                self.logger.info(\"\")\n                self.logger.info(\"-\" * 80)\n                self.logger.info(\"NEW BEST CONFIGURATION FOUND\")\n                self.logger.info(\"-\" * 80)\n                self.logger.info(f\"  Configuration ID: {config_id}/{total_configs}\")\n                self.logger.info(f\"  Mean Accuracy: {mean_accuracy:.4f} (±{std_accuracy:.4f})\")\n                self.logger.info(f\"  Accuracy Range: [{min_accuracy:.4f}, {max_accuracy:.4f}]\")\n                self.logger.info(\"-\" * 80)\n\n            elapsed_time = (datetime.now() - start_time).total_seconds()\n            avg_time_per_config = elapsed_time / config_id\n            remaining_configs = total_configs - config_id\n            eta_seconds = avg_time_per_config * remaining_configs\n            eta_str = str(timedelta(seconds=int(eta_seconds)))\n\n            pbar.set_postfix_str(\n                f\"Best: {best_accuracy:.4f} | Current: {mean_accuracy:.4f} | ETA: {eta_str}\",\n                refresh=True\n            )\n\n        pbar.close()\n\n        total_time = (datetime.now() - start_time).total_seconds()\n        total_time_str = str(timedelta(seconds=int(total_time)))\n\n        print_section_header(\"GRID SEARCH COMPLETED\", self.logger)\n        self.logger.info(\"\")\n        self.logger.info(\"Summary of Grid Search Results:\")\n        self.logger.info(\"-\" * 80)\n        self.logger.info(f\"Total configurations evaluated: {total_configs}\")\n        self.logger.info(f\"Total training time: {total_time_str}\")\n        self.logger.info(f\"Average time per configuration: {total_time/total_configs:.1f}s\")\n        self.logger.info(\"\")\n        self.logger.info(f\"Best configuration ID: #{best_config_id}\")\n        self.logger.info(f\"Best cross-validation accuracy: {best_accuracy:.4f}\")\n        self.logger.info(\"-\" * 80)\n        self.logger.info(\"\")\n        self.logger.info(\"Optimal Hyperparameters:\")\n        self.logger.info(\"-\" * 80)\n        for param_name, param_value in best_params.items():\n            self.logger.info(f\"  {param_name:22s}: {param_value}\")\n        self.logger.info(\"-\" * 80)\n\n        self.logger.info(\"\")\n        self.logger.info(\"Top 5 Configurations by Accuracy:\")\n        self.logger.info(\"-\" * 80)\n        sorted_results = sorted(grid_results, key=lambda x: x['mean_accuracy'], reverse=True)\n        for rank, result in enumerate(sorted_results[:5], 1):\n            self.logger.info(\n                f\"  {rank}. Config #{result['configuration_id']:4d} | \"\n                f\"Accuracy: {result['mean_accuracy']:.4f} (±{result['std_accuracy']:.4f}) | \"\n                f\"{result['optimizer']:4s}\"\n            )\n        self.logger.info(\"-\" * 80)\n\n        return {\n            'best_params': best_params,\n            'best_accuracy': best_accuracy,\n            'best_configuration_id': best_config_id,\n            'grid_results': grid_results,\n            'total_training_time_seconds': total_time\n        }\n\n    def train_final_models_with_best_config(\n        self,\n        X: pd.DataFrame,\n        y: pd.Series,\n        sira: pd.Series,\n        best_params: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Train final models with optimal hyperparameters and save detailed results.\"\"\"\n        print_section_header(\"TRAINING FINAL MODELS WITH OPTIMAL HYPERPARAMETERS\", self.logger)\n\n        y_numeric = y.replace({'D': 0, 'E': 1})\n        kf = KFold(n_splits=self.config.N_FOLDS, shuffle=False)\n\n        fold_results = []\n        fold_full_results = []\n        conf_matrices = []\n        fold_models = []\n        fold_scalers = []\n        fold_detailed_results = []\n\n        detailed_results_dir = os.path.join(self.config.OUTPUT_PATH, 'best_config_detailed_results')\n        Path(detailed_results_dir).mkdir(parents=True, exist_ok=True)\n\n        detailed_results_file = os.path.join(detailed_results_dir, 'fold_by_fold_results.txt')\n\n        with open(detailed_results_file, 'w', encoding='utf-8') as f:\n            f.write(\"=\" * 80 + \"\\n\")\n            f.write(\"BEST CONFIGURATION - DETAILED FOLD-BY-FOLD RESULTS\\n\")\n            f.write(\"=\" * 80 + \"\\n\\n\")\n\n            f.write(\"Optimal Hyperparameters:\\n\")\n            f.write(\"-\" * 80 + \"\\n\")\n            for param_name, param_value in best_params.items():\n                f.write(f\"  {param_name:22s}: {param_value}\\n\")\n            f.write(\"-\" * 80 + \"\\n\\n\")\n\n            for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n                self.logger.info(f\"\\nTraining Fold {fold}/{self.config.N_FOLDS}\")\n\n                X_train = X.iloc[train_idx].copy()\n                X_test = X.iloc[test_idx].copy()\n                y_train = y_numeric.iloc[train_idx].copy()\n                y_test = y_numeric.iloc[test_idx].copy()\n                sira_test = sira.iloc[test_idx]\n\n                scaler = StandardScaler()\n                X_train_scaled = scaler.fit_transform(X_train)\n                X_test_scaled = scaler.transform(X_test)\n\n                X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n                y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n                X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n                y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n\n                train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n                train_loader = TorchDataLoader(\n                    train_dataset,\n                    batch_size=best_params['batch_size'],\n                    shuffle=True\n                )\n\n                val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n                val_loader = TorchDataLoader(\n                    val_dataset,\n                    batch_size=best_params['batch_size'],\n                    shuffle=False\n                )\n\n                model = KAN(\n                    layers_hidden=best_params['layers_hidden'],\n                    grid_size=best_params['grid_size'],\n                    spline_order=best_params['spline_order'],\n                    scale_base=best_params['scale_base'],\n                    scale_spline=best_params['scale_spline']\n                )\n\n                if best_params['optimizer'] == 'Adam':\n                    optimizer = optim.Adam(model.parameters(), lr=self.config.LEARNING_RATE)\n                else:\n                    optimizer = optim.SGD(model.parameters(), lr=self.config.LEARNING_RATE, momentum=0.9)\n\n                criterion = torch.nn.BCEWithLogitsLoss()\n\n                history = self.train_single_fold(\n                    model, train_loader, val_loader,\n                    optimizer, criterion, fold, verbose=False\n                )\n\n                pred_test, prob_test, metrics_test = self.evaluate_model(\n                    model, X_test_tensor, y_test_tensor\n                )\n\n                cm_test = confusion_matrix(y_test_tensor.cpu().numpy(), pred_test)\n                conf_matrices.append(cm_test)\n\n                y_test_labels = ['D' if x == 0 else 'E' for x in y_test_tensor.cpu().numpy()]\n                pred_test_labels = ['D' if x == 0 else 'E' for x in pred_test]\n                class_report = classification_report(y_test_labels, pred_test_labels, target_names=['D', 'E'])\n\n                f.write(f\"\\nFold {fold}:\\n\")\n                f.write(\"-\" * 80 + \"\\n\")\n                f.write(f\"Accuracy:     {metrics_test['Accuracy']:.16f}\\n\")\n                f.write(f\"Precision:    {metrics_test['Precision']:.16f}\\n\")\n                f.write(f\"Recall:       {metrics_test['Recall']:.16f}\\n\")\n                f.write(f\"Specificity:  {metrics_test['Specificity']:.16f}\\n\")\n                f.write(f\"F1-Score:     {metrics_test['F1-Score']:.16f}\\n\")\n                f.write(f\"MCC:          {metrics_test['MCC']:.16f}\\n\")\n                f.write(\"-\" * 80 + \"\\n\")\n                f.write(f\"\\nClassification Report:\\n\")\n                f.write(class_report)\n                f.write(f\"\\nConfusion Matrix:\\n\")\n                f.write(f\" {cm_test}\\n\")\n                f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n\n                fold_results.append({\n                    'Sira': sira_test.values,\n                    'Predicted': pred_test,\n                    'Actual': y_test_tensor.cpu().numpy().astype(int),\n                    'Probability': prob_test,\n                    'Fold': fold\n                })\n\n                fold_detailed_results.append({\n                    'fold': fold,\n                    'accuracy': metrics_test['Accuracy'],\n                    'precision': metrics_test['Precision'],\n                    'recall': metrics_test['Recall'],\n                    'specificity': metrics_test['Specificity'],\n                    'f1_score': metrics_test['F1-Score'],\n                    'mcc': metrics_test['MCC'],\n                    'confusion_matrix': cm_test,\n                    'classification_report': class_report,\n                    'metrics': metrics_test\n                })\n\n                self.logger.info(f\"  Test fold accuracy: {metrics_test['Accuracy']:.4f}\")\n\n                X_full_scaled = scaler.transform(X)\n                X_full_tensor = torch.tensor(X_full_scaled, dtype=torch.float32)\n                y_full_tensor = torch.tensor(y_numeric.values, dtype=torch.float32)\n\n                pred_full, prob_full, metrics_full = self.evaluate_model(\n                    model, X_full_tensor, y_full_tensor\n                )\n\n                cm_full = confusion_matrix(y_full_tensor.cpu().numpy(), pred_full)\n\n                fold_full_results.append({\n                    'Fold': fold,\n                    'Predictions': pred_full,\n                    'Probabilities': prob_full,\n                    'Confusion_Matrix': cm_full,\n                    **metrics_full\n                })\n\n                fold_models.append(model)\n                fold_scalers.append(scaler)\n\n                self.logger.info(f\"  Full dataset accuracy: {metrics_full['Accuracy']:.4f}\")\n\n            total_cm = np.sum(conf_matrices, axis=0)\n            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n            f.write(\"TOTAL CONFUSION MATRIX (All Folds Combined):\\n\")\n            f.write(\"=\" * 80 + \"\\n\")\n            f.write(f\" {total_cm}\\n\")\n            f.write(\"\\n\")\n\n            tn, fp, fn, tp = total_cm.ravel()\n            overall_accuracy = (tp + tn) / (tp + tn + fp + fn)\n            overall_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n            overall_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n            overall_specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n            overall_f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n\n            overall_mcc = matthews_corrcoef(\n                np.concatenate([fold_data['Actual'] for fold_data in fold_results]),\n                np.concatenate([fold_data['Predicted'] for fold_data in fold_results])\n            )\n\n            f.write(\"\\nOVERALL METRICS (Cross-Validation):\\n\")\n            f.write(\"-\" * 80 + \"\\n\")\n            f.write(f\"Accuracy:     {overall_accuracy:.16f}\\n\")\n            f.write(f\"Precision:    {overall_precision:.16f}\\n\")\n            f.write(f\"Recall:       {overall_recall:.16f}\\n\")\n            f.write(f\"Specificity:  {overall_specificity:.16f}\\n\")\n            f.write(f\"F1-Score:     {overall_f1:.16f}\\n\")\n            f.write(f\"MCC:          {overall_mcc:.16f}\\n\")\n            f.write(\"-\" * 80 + \"\\n\")\n\n        self.logger.info(f\"\\nDetailed results saved to: {detailed_results_file}\")\n\n        best_fold_idx = max(range(len(fold_detailed_results)),\n                           key=lambda i: fold_detailed_results[i]['accuracy'])\n        best_fold_num = best_fold_idx + 1\n        best_fold_accuracy = fold_detailed_results[best_fold_idx]['accuracy']\n\n        self.logger.info(\"\")\n        self.logger.info(\"=\" * 80)\n        self.logger.info(f\"BEST FOLD IDENTIFIED: Fold {best_fold_num}\")\n        self.logger.info(f\"Best Fold Test Accuracy: {best_fold_accuracy:.4f}\")\n        self.logger.info(\"=\" * 80)\n\n        best_model_dir = os.path.join(self.config.OUTPUT_PATH, 'best_model')\n        Path(best_model_dir).mkdir(parents=True, exist_ok=True)\n\n        best_model_path = os.path.join(best_model_dir, f'best_model_fold{best_fold_num}.pth')\n        best_scaler_path = os.path.join(best_model_dir, f'best_scaler_fold{best_fold_num}.pkl')\n        best_params_path = os.path.join(best_model_dir, 'best_hyperparameters.pkl')\n\n        torch.save(fold_models[best_fold_idx].state_dict(), best_model_path)\n        with open(best_scaler_path, 'wb') as f:\n            pickle.dump(fold_scalers[best_fold_idx], f)\n        with open(best_params_path, 'wb') as f:\n            pickle.dump(best_params, f)\n\n        self.logger.info(f\"Best model saved to: {best_model_path}\")\n        self.logger.info(f\"Best scaler saved to: {best_scaler_path}\")\n        self.logger.info(f\"Best hyperparameters saved to: {best_params_path}\")\n\n        best_model = fold_models[best_fold_idx]\n        best_scaler = fold_scalers[best_fold_idx]\n\n        X_full_scaled = best_scaler.transform(X)\n        X_full_tensor = torch.tensor(X_full_scaled, dtype=torch.float32)\n        y_full_tensor = torch.tensor(y_numeric.values, dtype=torch.float32)\n\n        pred_full_best, prob_full_best, metrics_full_best = self.evaluate_model(\n            best_model, X_full_tensor, y_full_tensor\n        )\n\n        best_predictions_df = pd.DataFrame({\n            'Sira': sira.values,\n            'Actual': y.values,\n            'Predicted': ['D' if p == 0 else 'E' for p in pred_full_best],\n            'Probability_Class_0': 1 - prob_full_best,\n            'Probability_Class_1': prob_full_best\n        })\n\n        best_predictions_file = os.path.join(best_model_dir, 'best_model_full_dataset_predictions.xlsx')\n        best_predictions_df.to_excel(best_predictions_file, index=False)\n        self.logger.info(f\"Best model predictions saved to: {best_predictions_file}\")\n\n        cm_best_full = confusion_matrix(y_full_tensor.cpu().numpy(), pred_full_best)\n        cm_best_file = os.path.join(best_model_dir, 'best_model_confusion_matrix.png')\n        save_confusion_matrix(\n            cm_best_full, cm_best_file, self.config.CLASS_NAMES,\n            f'Best Model (Fold {best_fold_num}) - Full Dataset Confusion Matrix'\n        )\n        self.logger.info(f\"Best model confusion matrix saved to: {cm_best_file}\")\n\n        best_model_metrics_file = os.path.join(best_model_dir, 'best_model_metrics.txt')\n        with open(best_model_metrics_file, 'w', encoding='utf-8') as f:\n            f.write(\"=\" * 80 + \"\\n\")\n            f.write(f\"BEST MODEL - FOLD {best_fold_num}\\n\")\n            f.write(\"=\" * 80 + \"\\n\\n\")\n\n            f.write(\"Test Fold Metrics:\\n\")\n            f.write(\"-\" * 80 + \"\\n\")\n            f.write(f\"Accuracy:     {fold_detailed_results[best_fold_idx]['accuracy']:.16f}\\n\")\n            f.write(f\"Precision:    {fold_detailed_results[best_fold_idx]['precision']:.16f}\\n\")\n            f.write(f\"Recall:       {fold_detailed_results[best_fold_idx]['recall']:.16f}\\n\")\n            f.write(f\"Specificity:  {fold_detailed_results[best_fold_idx]['specificity']:.16f}\\n\")\n            f.write(f\"F1-Score:     {fold_detailed_results[best_fold_idx]['f1_score']:.16f}\\n\")\n            f.write(f\"MCC:          {fold_detailed_results[best_fold_idx]['mcc']:.16f}\\n\")\n            f.write(\"-\" * 80 + \"\\n\\n\")\n\n            f.write(\"Full Dataset Metrics:\\n\")\n            f.write(\"-\" * 80 + \"\\n\")\n            f.write(f\"Accuracy:     {metrics_full_best['Accuracy']:.16f}\\n\")\n            f.write(f\"Precision:    {metrics_full_best['Precision']:.16f}\\n\")\n            f.write(f\"Recall:       {metrics_full_best['Recall']:.16f}\\n\")\n            f.write(f\"Specificity:  {metrics_full_best['Specificity']:.16f}\\n\")\n            f.write(f\"F1-Score:     {metrics_full_best['F1-Score']:.16f}\\n\")\n            f.write(f\"MCC:          {metrics_full_best['MCC']:.16f}\\n\")\n            f.write(\"-\" * 80 + \"\\n\\n\")\n\n            f.write(\"Confusion Matrix (Full Dataset):\\n\")\n            f.write(f\" {cm_best_full}\\n\")\n\n        self.logger.info(f\"Best model detailed metrics saved to: {best_model_metrics_file}\")\n\n        return {\n            'fold_results': fold_results,\n            'fold_full_results': fold_full_results,\n            'conf_matrices': conf_matrices,\n            'fold_detailed_results': fold_detailed_results,\n            'best_fold_num': best_fold_num,\n            'best_fold_accuracy': best_fold_accuracy,\n            'best_model_path': best_model_path,\n            'best_scaler_path': best_scaler_path,\n            'best_predictions': best_predictions_df,\n            'best_model_metrics': metrics_full_best\n        }\n\n\nclass ResultsManager:\n    \"\"\"Generate comprehensive reports and visualizations.\"\"\"\n\n    def __init__(self, config: Config, logger: logging.Logger):\n        self.config = config\n        self.logger = logger\n\n    def save_results(\n        self,\n        grid_search_results: Dict,\n        training_results: Dict,\n        X: pd.DataFrame,\n        y: pd.Series,\n        sira: pd.Series\n    ) -> None:\n        \"\"\"Save all results and generate visualizations.\"\"\"\n        print_section_header(\"SAVING RESULTS AND GENERATING REPORTS\", self.logger)\n\n        output_excel = os.path.join(self.config.OUTPUT_PATH, 'kan_comprehensive_results.xlsx')\n\n        with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:\n            grid_df = pd.DataFrame(grid_search_results['grid_results'])\n            grid_df = grid_df.sort_values('mean_accuracy', ascending=False)\n            grid_df.to_excel(writer, sheet_name='Grid_Search_Results', index=False)\n\n            best_params_df = pd.DataFrame([grid_search_results['best_params']])\n            best_params_df.to_excel(writer, sheet_name='Optimal_Parameters', index=False)\n\n            cv_results_list = []\n            for fold_data in training_results['fold_results']:\n                fold_df = pd.DataFrame({\n                    'Sira': fold_data['Sira'],\n                    'Predicted': fold_data['Predicted'],\n                    'Actual': fold_data['Actual'],\n                    'Probability': fold_data['Probability'],\n                    'Fold': fold_data['Fold']\n                })\n                cv_results_list.append(fold_df)\n\n            cv_results_combined = pd.concat(cv_results_list, ignore_index=True)\n            cv_results_combined = cv_results_combined.sort_values('Sira').reset_index(drop=True)\n            cv_results_combined.to_excel(writer, sheet_name='CV_Predictions', index=False)\n\n            full_results_df = pd.DataFrame([{\n                'Fold': r['Fold'],\n                'Accuracy': r['Accuracy'],\n                'Precision': r['Precision'],\n                'Recall': r['Recall'],\n                'Specificity': r['Specificity'],\n                'F1-Score': r['F1-Score'],\n                'MCC': r['MCC']\n            } for r in training_results['fold_full_results']])\n            full_results_df.to_excel(writer, sheet_name='Full_Dataset_Per_Fold', index=False)\n\n            y_numeric = y.replace({'D': 0, 'E': 1})\n            total_cm = np.sum(training_results['conf_matrices'], axis=0)\n            tn, fp, fn, tp = total_cm.ravel()\n\n            cv_summary = pd.DataFrame({\n                'Metric': ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score', 'MCC'],\n                'Value': [\n                    (tp + tn) / (tp + tn + fp + fn),\n                    tp / (tp + fp) if (tp + fp) > 0 else 0,\n                    tp / (tp + fn) if (tp + fn) > 0 else 0,\n                    tn / (tn + fp) if (tn + fp) > 0 else 0,\n                    2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0,\n                    matthews_corrcoef(\n                        cv_results_combined['Actual'].values,\n                        cv_results_combined['Predicted'].values\n                    )\n                ]\n            })\n            cv_summary.to_excel(writer, sheet_name='CV_Summary', index=False)\n\n            full_summary = pd.DataFrame({\n                'Metric': ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score', 'MCC'],\n                'Mean': [full_results_df[m].mean() for m in ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score', 'MCC']],\n                'Std': [full_results_df[m].std() for m in ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score', 'MCC']],\n                'Min': [full_results_df[m].min() for m in ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score', 'MCC']],\n                'Max': [full_results_df[m].max() for m in ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score', 'MCC']]\n            })\n            full_summary.to_excel(writer, sheet_name='Full_Dataset_Summary', index=False)\n\n            best_model_info = pd.DataFrame([{\n                'Best_Fold': training_results['best_fold_num'],\n                'Best_Fold_Test_Accuracy': training_results['best_fold_accuracy'],\n                'Best_Model_Full_Dataset_Accuracy': training_results['best_model_metrics']['Accuracy'],\n                'Best_Model_Precision': training_results['best_model_metrics']['Precision'],\n                'Best_Model_Recall': training_results['best_model_metrics']['Recall'],\n                'Best_Model_Specificity': training_results['best_model_metrics']['Specificity'],\n                'Best_Model_F1_Score': training_results['best_model_metrics']['F1-Score'],\n                'Best_Model_MCC': training_results['best_model_metrics']['MCC']\n            }])\n            best_model_info.to_excel(writer, sheet_name='Best_Model_Info', index=False)\n\n            for fold_data in training_results['fold_full_results']:\n                fold_num = fold_data['Fold']\n                fold_pred_df = pd.DataFrame({\n                    'Sira': sira.values,\n                    'Predicted': fold_data['Predictions'],\n                    'Actual': y_numeric.values,\n                    'Probability': fold_data['Probabilities']\n                })\n                fold_pred_df.to_excel(writer, sheet_name=f'Fold{fold_num}_Full_Predictions', index=False)\n\n        self.logger.info(f\"Comprehensive results saved to: {output_excel}\")\n\n        self._generate_visualizations(training_results)\n\n    def _generate_visualizations(self, training_results: Dict) -> None:\n        \"\"\"Generate confusion matrices and visualizations.\"\"\"\n        self.logger.info(\"Generating visualizations...\")\n\n        total_cm = np.sum(training_results['conf_matrices'], axis=0)\n        cm_file = os.path.join(self.config.OUTPUT_PATH, 'confusion_matrix_cv_total.png')\n        save_confusion_matrix(\n            total_cm, cm_file, self.config.CLASS_NAMES,\n            'Cross-Validation Total Confusion Matrix'\n        )\n\n        for fold_data in training_results['fold_full_results']:\n            fold_num = fold_data['Fold']\n            cm = fold_data['Confusion_Matrix']\n            cm_file = os.path.join(\n                self.config.OUTPUT_PATH,\n                f'confusion_matrix_fold{fold_num}_full_dataset.png'\n            )\n            save_confusion_matrix(\n                cm, cm_file, self.config.CLASS_NAMES,\n                f'Fold {fold_num} - Full Dataset Confusion Matrix'\n            )\n\n        self.logger.info(\"All visualizations generated successfully\")\n\n\ndef main():\n    \"\"\"Main execution pipeline for KAN training.\"\"\"\n\n    logger = setup_logging(Config.OUTPUT_PATH)\n    Config.display_config(logger)\n\n    try:\n        data_manager = DataManager(Config, logger)\n        X, y, sira = data_manager.load_data()\n\n        trainer = KANTrainer(Config, logger)\n\n        grid_search_results = trainer.grid_search(X, y, sira)\n\n        grid_file = os.path.join(Config.OUTPUT_PATH, 'grid_search_results.xlsx')\n        grid_df = pd.DataFrame(grid_search_results['grid_results'])\n        grid_df = grid_df.sort_values('mean_accuracy', ascending=False)\n        grid_df.to_excel(grid_file, index=False)\n        logger.info(f\"\\nGrid search results saved to: {grid_file}\")\n\n        training_results = trainer.train_final_models_with_best_config(\n            X, y, sira,\n            grid_search_results['best_params']\n        )\n\n        results_manager = ResultsManager(Config, logger)\n        results_manager.save_results(\n            grid_search_results,\n            training_results,\n            X, y, sira\n        )\n\n        print_section_header(\"TRAINING COMPLETED SUCCESSFULLY\", logger)\n        logger.info(f\"\\nAll outputs saved to: {Config.OUTPUT_PATH}\")\n        logger.info(f\"Best model saved to: {training_results['best_model_path']}\")\n        logger.info(f\"Best fold: {training_results['best_fold_num']} (Accuracy: {training_results['best_fold_accuracy']:.4f})\")\n\n    except Exception as e:\n        logger.error(f\"Training failed: {str(e)}\", exc_info=True)\n        raise\n\n    finally:\n        logger.info(\"\")\n        logger.info(\"=\" * 80)\n        logger.info(f\"Session ended: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        logger.info(\"=\" * 80)\n\n\nif __name__ == \"__main__\":\n    main()",
   "id": "db1a7d1d6c3d08e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}