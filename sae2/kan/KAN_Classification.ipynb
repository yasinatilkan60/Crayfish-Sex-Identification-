{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import confusion_matrix\nfrom itertools import product\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Mount Google Drive (skip if already mounted)\nfrom google.colab import drive\ndrive.mount('/content/drive')\n# Add custom KAN paths\nimport sys\nsys.path.append('/content/drive/MyDrive/packages/efficient_kan')\nsys.path.append('/content/drive/MyDrive/packages/fastkan')\n\n# Import KAN and FastKAN classes\nfrom kan import *\nfrom fastkan import *"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a7d1d6c3d08e3",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nKolmogorov-Arnold Networks (KAN) for Crayfish Sex Classification\n================================================================\n\nOriginal Paper Authors:\n    Yasin Atilkan¹, Berk Kirik², Eren Tuna Acikbas³, Fatih Ekinci⁴,\n    Koray Acici¹'⁴, Tunc Asuroglu⁵'⁶*, Recep Benzer⁷,\n    Mehmet Serdar Guzel⁴'⁸, Semra Benzer⁹\n\nAffiliations:\n    ¹ Department of Artificial Intelligence and Data Engineering, Ankara University, Turkey\n    ² Department of Biomedical Engineering, Ankara University, Turkey\n    ³ Department of Petroleum and Natural Gas Engineering, Middle East Technical University, Turkey\n    ⁴ Institute of Artificial Intelligence, Ankara University, Turkey\n    ⁵ Faculty of Medicine and Health Technology, Tampere University, Finland\n    ⁶ VTT Technical Research Centre of Finland, Finland\n    ⁷ Department of Management Information System, Ankara Medipol University, Turkey\n    ⁸ Department of Computer Engineering, Ankara University, Turkey\n    ⁹ Department of Science Education, Gazi University, Turkey\n    * Corresponding author\n\nImplementation follows the methodology from:\n\"Enhancing Crayfish Sex Identification with Kolmogorov-Arnold Networks\nand Stacked Autoencoders\" (Atilkan et al., 2025)\n\nKey Features:\n- Grid search for optimal hyperparameter selection\n- Pre-split train/test data from Stacked Autoencoder\n- Per-split data standardization (prevents data leakage)\n- Early stopping mechanism for overfitting mitigation\n- Comprehensive evaluation metrics\n- Best model identification and storage\n\nImplementation Date: 2025\n\"\"\"\n\n\nimport os\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Tuple, Any\nimport warnings\nfrom pathlib import Path\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader as TorchDataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    confusion_matrix, accuracy_score, precision_score,\n    recall_score, f1_score, matthews_corrcoef, classification_report\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import product\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore')\n\n\nclass Config:\n    \"\"\"Configuration class for KAN model training and evaluation.\"\"\"\n\n    # Data paths - Stacked Autoencoder extracted features\n    TRAIN_FEATURES_PATH = 'train_features_stackedautoencoder.npy'\n    TRAIN_LABELS_PATH = 'train_labels_stackedautoencoder.npy'\n    TEST_FEATURES_PATH = 'test_features_stackedautoencoder.npy'\n    TEST_LABELS_PATH = 'test_labels_stackedautoencoder.npy'\n    \n    OUTPUT_PATH = '/content/drive/MyDrive/crawfish_results/ml_results/kan_grid_search_sae'\n\n    GRID_SEARCH_PARAMS = {\n        'layers_hidden': [\n            [18432, 1024, 512, 256, 128, 64, 32, 1],\n            [18432, 512, 256, 128, 64, 32, 1],\n            [18432, 256, 128, 64, 32, 1],\n            [18432, 64, 32, 1]\n        ],\n        'grid_size': [8, 7, 6, 5, 4],\n        'learning_rate': [0.0005, 0.005, 0.001, 0.0001],\n        'spline_order': [5, 4, 3],\n        'scale_base': [1.0, 2.0, 3.0],\n        'scale_spline': [1.0, 2.0],\n        'batch_size': [128, 64, 32],\n        'optimizer': ['Adam', 'SGD']\n    }\n\n    NUM_EPOCHS = 50\n    EARLY_STOPPING_PATIENCE = 10\n    EARLY_STOPPING_DELTA = 0.0\n\n    FIG_SIZE = (10, 8)\n    DPI = 300\n    CLASS_NAMES = ['Female (D)', 'Male (E)']\n\n    @classmethod\n    def display_config(cls, logger: logging.Logger) -> None:\n        \"\"\"Display current configuration.\"\"\"\n        logger.info(\"Configuration Summary:\")\n        logger.info(f\"  Total hyperparameter combinations: {cls._get_total_combinations()}\")\n        logger.info(f\"  Learning rates: {cls.GRID_SEARCH_PARAMS['learning_rate']}\")\n        logger.info(f\"  Maximum epochs: {cls.NUM_EPOCHS}\")\n        logger.info(f\"  Early stopping patience: {cls.EARLY_STOPPING_PATIENCE} epochs\")\n        logger.info(f\"  Early stopping delta: {cls.EARLY_STOPPING_DELTA}\")\n\n    @classmethod\n    def _get_total_combinations(cls) -> int:\n        \"\"\"Calculate total number of hyperparameter combinations.\"\"\"\n        return int(np.prod([len(v) for v in cls.GRID_SEARCH_PARAMS.values()]))\n\n\ndef setup_logging(output_path: str) -> logging.Logger:\n    \"\"\"Setup comprehensive logging system with file and console handlers.\"\"\"\n    Path(output_path).mkdir(parents=True, exist_ok=True)\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(output_path, f'kan_training_{timestamp}.log')\n\n    logger = logging.getLogger('KAN_Training')\n    logger.setLevel(logging.DEBUG)\n    logger.handlers = []\n    logger.propagate = False\n\n    file_handler = logging.FileHandler(log_file, encoding='utf-8')\n    file_handler.setLevel(logging.DEBUG)\n    file_formatter = logging.Formatter(\n        '%(asctime)s | %(levelname)-8s | %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n    file_handler.setFormatter(file_formatter)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.INFO)\n    console_formatter = logging.Formatter('%(message)s')\n    console_handler.setFormatter(console_formatter)\n\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n\n    logger.info(\"=\" * 80)\n    logger.info(\"KAN Training for Crayfish Sex Classification (Stacked Autoencoder)\")\n    logger.info(\"Based on: Atilkan et al. (2025)\")\n    logger.info(\"=\" * 80)\n    logger.info(f\"Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    logger.info(f\"Log file: {log_file}\")\n    logger.info(\"=\" * 80)\n\n    return logger\n\n\nclass EarlyStopping:\n    \"\"\"Early stopping mechanism to prevent overfitting.\"\"\"\n\n    def __init__(self, patience: int = 10, verbose: bool = False, delta: float = 0.0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.delta = delta\n        self.best_model_state = None\n\n    def __call__(self, val_loss: float, model: torch.nn.Module) -> None:\n        \"\"\"Check if early stopping criteria is met.\"\"\"\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.best_model_state = model.state_dict()\n        elif val_loss > self.best_loss - self.delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n                model.load_state_dict(self.best_model_state)\n        else:\n            self.best_loss = val_loss\n            self.best_model_state = model.state_dict()\n            self.counter = 0\n\n    def reset(self) -> None:\n        \"\"\"Reset early stopping state.\"\"\"\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.best_model_state = None\n\n\ndef calculate_specificity(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculate specificity (true negative rate).\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    if cm.size == 1:\n        return 1.0\n    tn, fp, fn, tp = cm.ravel()\n    return tn / (tn + fp) if (tn + fp) > 0 else 0.0\n\n\ndef calculate_all_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n    \"\"\"Calculate all evaluation metrics.\"\"\"\n    return {\n        'Accuracy': accuracy_score(y_true, y_pred),\n        'Precision': precision_score(y_true, y_pred, zero_division=0),\n        'Recall': recall_score(y_true, y_pred, zero_division=0),\n        'Specificity': calculate_specificity(y_true, y_pred),\n        'F1-Score': f1_score(y_true, y_pred, zero_division=0),\n        'MCC': matthews_corrcoef(y_true, y_pred)\n    }\n\n\ndef save_confusion_matrix(\n    cm: np.ndarray,\n    filename: str,\n    class_names: List[str],\n    title: str = 'Confusion Matrix'\n) -> None:\n    \"\"\"Save confusion matrix as high-quality PNG image.\"\"\"\n    plt.figure(figsize=Config.FIG_SIZE)\n\n    sns.heatmap(\n        cm, annot=True, fmt='d', cmap='Blues',\n        xticklabels=class_names, yticklabels=class_names,\n        cbar=True, square=True, linewidths=1, linecolor='gray'\n    )\n\n    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n    plt.ylabel('True Label', fontsize=14, fontweight='bold')\n    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(filename, dpi=Config.DPI, bbox_inches='tight')\n    plt.close()\n\n\ndef print_section_header(title: str, logger: logging.Logger, level: int = 1) -> None:\n    \"\"\"Print professional section header.\"\"\"\n    logger.info(\"\")\n\n    if level == 1:\n        logger.info(\"=\" * 80)\n        logger.info(title)\n        logger.info(\"=\" * 80)\n    else:\n        logger.info(\"-\" * 80)\n        logger.info(title)\n        logger.info(\"-\" * 80)\n\n\ndef format_metrics_table(metrics: Dict[str, float]) -> str:\n    \"\"\"Format metrics dictionary as readable table.\"\"\"\n    lines = []\n    lines.append(\"-\" * 40)\n    for metric, value in metrics.items():\n        lines.append(f\"  {metric:<20} {value:>8.4f}\")\n    lines.append(\"-\" * 40)\n    return \"\\n\".join(lines)\n\n\nclass DataManager:\n    \"\"\"Handle data loading and preprocessing operations.\"\"\"\n\n    def __init__(self, config: Config, logger: logging.Logger):\n        self.config = config\n        self.logger = logger\n\n    def load_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Load pre-split dataset from Stacked Autoencoder .npy files.\"\"\"\n        self.logger.info(\"Loading pre-split data from Stacked Autoencoder...\")\n\n        try:\n            train_features = np.load(self.config.TRAIN_FEATURES_PATH)\n            train_labels = np.load(self.config.TRAIN_LABELS_PATH)\n            test_features = np.load(self.config.TEST_FEATURES_PATH)\n            test_labels = np.load(self.config.TEST_LABELS_PATH)\n            \n            self.logger.info(f\"Successfully loaded data:\")\n            self.logger.info(f\"  Train features shape: {train_features.shape}\")\n            self.logger.info(f\"  Train labels shape: {train_labels.shape}\")\n            self.logger.info(f\"  Test features shape: {test_features.shape}\")\n            self.logger.info(f\"  Test labels shape: {test_labels.shape}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to load data: {str(e)}\")\n            raise\n\n        self._log_dataset_info(train_labels, test_labels)\n        return train_features, train_labels, test_features, test_labels\n\n    def _log_dataset_info(self, train_labels: np.ndarray, test_labels: np.ndarray) -> None:\n        \"\"\"Log dataset statistics.\"\"\"\n        self.logger.info(\"\")\n        self.logger.info(\"Dataset Statistics:\")\n        self.logger.info(f\"  Total train samples: {len(train_labels)}\")\n        self.logger.info(f\"  Total test samples: {len(test_labels)}\")\n        \n        # Train class distribution\n        unique_train, counts_train = np.unique(train_labels, return_counts=True)\n        self.logger.info(\"  Train class distribution:\")\n        for cls, count in zip(unique_train, counts_train):\n            percentage = count / len(train_labels) * 100\n            self.logger.info(f\"    Class {cls}: {count:3d} samples ({percentage:5.1f}%)\")\n        \n        # Test class distribution\n        unique_test, counts_test = np.unique(test_labels, return_counts=True)\n        self.logger.info(\"  Test class distribution:\")\n        for cls, count in zip(unique_test, counts_test):\n            percentage = count / len(test_labels) * 100\n            self.logger.info(f\"    Class {cls}: {count:3d} samples ({percentage:5.1f}%)\")\n\n\nclass KANTrainer:\n    \"\"\"KAN model training with comprehensive grid search.\"\"\"\n\n    def __init__(self, config: Config, logger: logging.Logger):\n        self.config = config\n        self.logger = logger\n\n    def train_model(\n        self,\n        model: torch.nn.Module,\n        train_loader: TorchDataLoader,\n        val_loader: TorchDataLoader,\n        optimizer: torch.optim.Optimizer,\n        criterion: torch.nn.Module,\n        verbose: bool = False\n    ) -> Dict[str, Any]:\n        \"\"\"Train model with early stopping.\"\"\"\n        early_stopping = EarlyStopping(\n            patience=self.config.EARLY_STOPPING_PATIENCE,\n            verbose=verbose,\n            delta=self.config.EARLY_STOPPING_DELTA\n        )\n\n        history = {\n            'train_loss': [],\n            'val_loss': [],\n            'epochs_trained': 0,\n            'early_stopped': False\n        }\n\n        for epoch in range(self.config.NUM_EPOCHS):\n            model.train()\n            train_loss = 0.0\n            for X_batch, y_batch in train_loader:\n                optimizer.zero_grad()\n                outputs = model(X_batch).squeeze()\n                loss = criterion(outputs, y_batch)\n                loss.backward()\n                optimizer.step()\n                train_loss += loss.item()\n\n            avg_train_loss = train_loss / len(train_loader)\n            history['train_loss'].append(avg_train_loss)\n\n            model.eval()\n            val_loss = 0.0\n            with torch.no_grad():\n                for X_val, y_val in val_loader:\n                    val_outputs = model(X_val).squeeze()\n                    loss = criterion(val_outputs, y_val)\n                    val_loss += loss.item()\n\n            avg_val_loss = val_loss / len(val_loader)\n            history['val_loss'].append(avg_val_loss)\n\n            early_stopping(avg_val_loss, model)\n            if early_stopping.early_stop:\n                history['epochs_trained'] = epoch + 1\n                history['early_stopped'] = True\n                break\n        else:\n            history['epochs_trained'] = self.config.NUM_EPOCHS\n\n        return history\n\n    def evaluate_model(\n        self,\n        model: torch.nn.Module,\n        X_tensor: torch.Tensor,\n        y_tensor: torch.Tensor\n    ) -> Tuple[np.ndarray, np.ndarray, Dict[str, float]]:\n        \"\"\"Evaluate model and calculate metrics.\"\"\"\n        model.eval()\n        with torch.no_grad():\n            outputs = model(X_tensor).squeeze()\n            probabilities = torch.sigmoid(outputs).cpu().numpy()\n            predictions = (probabilities > 0.5).astype(int)\n            actuals = y_tensor.cpu().numpy()\n\n        metrics = calculate_all_metrics(actuals, predictions)\n        return predictions, probabilities, metrics\n\n    def grid_search(\n        self,\n        train_features: np.ndarray,\n        train_labels: np.ndarray,\n        test_features: np.ndarray,\n        test_labels: np.ndarray\n    ) -> Dict[str, Any]:\n        \"\"\"Perform grid search for optimal hyperparameters using pre-split data.\"\"\"\n        print_section_header(\"HYPERPARAMETER OPTIMIZATION VIA GRID SEARCH\", self.logger)\n\n        param_combinations = list(product(\n            self.config.GRID_SEARCH_PARAMS['layers_hidden'],\n            self.config.GRID_SEARCH_PARAMS['grid_size'],\n            self.config.GRID_SEARCH_PARAMS['learning_rate'],\n            self.config.GRID_SEARCH_PARAMS['spline_order'],\n            self.config.GRID_SEARCH_PARAMS['scale_base'],\n            self.config.GRID_SEARCH_PARAMS['scale_spline'],\n            self.config.GRID_SEARCH_PARAMS['batch_size'],\n            self.config.GRID_SEARCH_PARAMS['optimizer']\n        ))\n\n        total_configs = len(param_combinations)\n        self.logger.info(f\"Total hyperparameter configurations: {total_configs}\")\n        self.logger.info(\"\")\n\n        # Standardize features\n        scaler = StandardScaler()\n        train_features_scaled = scaler.fit_transform(train_features)\n        test_features_scaled = scaler.transform(test_features)\n\n        # Convert to tensors\n        X_train_tensor = torch.tensor(train_features_scaled, dtype=torch.float32)\n        y_train_tensor = torch.tensor(train_labels, dtype=torch.float32)\n        X_test_tensor = torch.tensor(test_features_scaled, dtype=torch.float32)\n        y_test_tensor = torch.tensor(test_labels, dtype=torch.float32)\n\n        grid_results = []\n        best_accuracy = 0.0\n        best_params = None\n        best_config_id = None\n        best_scaler = None\n\n        start_time = datetime.now()\n\n        pbar = tqdm(\n            enumerate(param_combinations, 1),\n            total=total_configs,\n            desc=\"Grid Search Progress\",\n            ncols=120,\n            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n        )\n\n        for config_id, (layers_hidden, grid_size, learning_rate, spline_order,\n                       scale_base, scale_spline, batch_size, optimizer_name) in pbar:\n\n            self.logger.debug(\"\")\n            self.logger.debug(f\"Configuration ID: {config_id}/{total_configs}\")\n            self.logger.debug(f\"  Architecture: {layers_hidden}\")\n            self.logger.debug(f\"  Grid size: {grid_size}, Spline order: {spline_order}\")\n            self.logger.debug(f\"  Learning rate: {learning_rate}\")\n            self.logger.debug(f\"  Scale base: {scale_base}, Scale spline: {scale_spline}\")\n            self.logger.debug(f\"  Batch size: {batch_size}, Optimizer: {optimizer_name}\")\n\n            # Create data loaders\n            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n            train_loader = TorchDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n            test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n            test_loader = TorchDataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n            try:\n                model = KAN(\n                    layers_hidden=list(layers_hidden),\n                    grid_size=grid_size,\n                    spline_order=spline_order,\n                    scale_base=scale_base,\n                    scale_spline=scale_spline\n                )\n            except NameError:\n                self.logger.error(\"KAN class not found. Please ensure KAN is properly imported.\")\n                raise\n\n            if optimizer_name == 'Adam':\n                optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n            else:\n                optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n\n            criterion = torch.nn.BCEWithLogitsLoss()\n\n            history = self.train_model(\n                model=model,\n                train_loader=train_loader,\n                val_loader=test_loader,\n                optimizer=optimizer,\n                criterion=criterion,\n                verbose=False\n            )\n\n            _, _, metrics = self.evaluate_model(model, X_test_tensor, y_test_tensor)\n            accuracy = metrics['Accuracy']\n\n            grid_results.append({\n                'configuration_id': config_id,\n                'architecture': str(layers_hidden),\n                'grid_size': grid_size,\n                'learning_rate': learning_rate,\n                'spline_order': spline_order,\n                'scale_base': scale_base,\n                'scale_spline': scale_spline,\n                'batch_size': batch_size,\n                'optimizer': optimizer_name,\n                'accuracy': accuracy,\n                'precision': metrics['Precision'],\n                'recall': metrics['Recall'],\n                'f1_score': metrics['F1-Score'],\n                'mcc': metrics['MCC']\n            })\n\n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                best_config_id = config_id\n                best_params = {\n                    'layers_hidden': list(layers_hidden),\n                    'grid_size': grid_size,\n                    'learning_rate': learning_rate,\n                    'spline_order': spline_order,\n                    'scale_base': scale_base,\n                    'scale_spline': scale_spline,\n                    'batch_size': batch_size,\n                    'optimizer': optimizer_name\n                }\n                best_scaler = scaler\n\n                self.logger.info(\"\")\n                self.logger.info(\"-\" * 80)\n                self.logger.info(\"NEW BEST CONFIGURATION FOUND\")\n                self.logger.info(\"-\" * 80)\n                self.logger.info(f\"  Configuration ID: {config_id}/{total_configs}\")\n                self.logger.info(f\"  Test Accuracy: {accuracy:.4f}\")\n                self.logger.info(f\"  Learning Rate: {learning_rate}\")\n                self.logger.info(\"-\" * 80)\n\n            elapsed_time = (datetime.now() - start_time).total_seconds()\n            avg_time_per_config = elapsed_time / config_id\n            remaining_configs = total_configs - config_id\n            eta_seconds = avg_time_per_config * remaining_configs\n            eta_str = str(timedelta(seconds=int(eta_seconds)))\n\n            pbar.set_postfix_str(\n                f\"Best: {best_accuracy:.4f} | Current: {accuracy:.4f} | ETA: {eta_str}\",\n                refresh=True\n            )\n\n        pbar.close()\n\n        total_time = (datetime.now() - start_time).total_seconds()\n        total_time_str = str(timedelta(seconds=int(total_time)))\n\n        print_section_header(\"GRID SEARCH COMPLETED\", self.logger)\n        self.logger.info(\"\")\n        self.logger.info(\"Summary of Grid Search Results:\")\n        self.logger.info(\"-\" * 80)\n        self.logger.info(f\"Total configurations evaluated: {total_configs}\")\n        self.logger.info(f\"Total training time: {total_time_str}\")\n        self.logger.info(f\"Average time per configuration: {total_time/total_configs:.1f}s\")\n        self.logger.info(\"\")\n        self.logger.info(f\"Best configuration ID: #{best_config_id}\")\n        self.logger.info(f\"Best test accuracy: {best_accuracy:.4f}\")\n        self.logger.info(\"-\" * 80)\n        self.logger.info(\"\")\n        self.logger.info(\"Optimal Hyperparameters:\")\n        self.logger.info(\"-\" * 80)\n        for param_name, param_value in best_params.items():\n            self.logger.info(f\"  {param_name:22s}: {param_value}\")\n        self.logger.info(\"-\" * 80)\n\n        self.logger.info(\"\")\n        self.logger.info(\"Top 5 Configurations by Accuracy:\")\n        self.logger.info(\"-\" * 80)\n        sorted_results = sorted(grid_results, key=lambda x: x['accuracy'], reverse=True)\n        for rank, result in enumerate(sorted_results[:5], 1):\n            self.logger.info(\n                f\"  {rank}. Config #{result['configuration_id']:4d} | \"\n                f\"Accuracy: {result['accuracy']:.4f} | \"\n                f\"LR: {result['learning_rate']} | {result['optimizer']:4s}\"\n            )\n        self.logger.info(\"-\" * 80)\n\n        return {\n            'best_params': best_params,\n            'best_accuracy': best_accuracy,\n            'best_configuration_id': best_config_id,\n            'grid_results': grid_results,\n            'total_training_time_seconds': total_time,\n            'scaler': best_scaler\n        }\n\n    def train_final_model(\n        self,\n        train_features: np.ndarray,\n        train_labels: np.ndarray,\n        test_features: np.ndarray,\n        test_labels: np.ndarray,\n        best_params: Dict[str, Any],\n        scaler: StandardScaler\n    ) -> Dict[str, Any]:\n        \"\"\"Train final model with optimal hyperparameters.\"\"\"\n        print_section_header(\"TRAINING FINAL MODEL WITH OPTIMAL HYPERPARAMETERS\", self.logger)\n\n        # Standardize features\n        train_features_scaled = scaler.fit_transform(train_features)\n        test_features_scaled = scaler.transform(test_features)\n\n        # Convert to tensors\n        X_train_tensor = torch.tensor(train_features_scaled, dtype=torch.float32)\n        y_train_tensor = torch.tensor(train_labels, dtype=torch.float32)\n        X_test_tensor = torch.tensor(test_features_scaled, dtype=torch.float32)\n        y_test_tensor = torch.tensor(test_labels, dtype=torch.float32)\n\n        # Create data loaders\n        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n        train_loader = TorchDataLoader(\n            train_dataset,\n            batch_size=best_params['batch_size'],\n            shuffle=True\n        )\n\n        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n        test_loader = TorchDataLoader(\n            test_dataset,\n            batch_size=best_params['batch_size'],\n            shuffle=False\n        )\n\n        # Create model\n        model = KAN(\n            layers_hidden=best_params['layers_hidden'],\n            grid_size=best_params['grid_size'],\n            spline_order=best_params['spline_order'],\n            scale_base=best_params['scale_base'],\n            scale_spline=best_params['scale_spline']\n        )\n\n        if best_params['optimizer'] == 'Adam':\n            optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n        else:\n            optimizer = optim.SGD(model.parameters(), lr=best_params['learning_rate'], momentum=0.9)\n\n        criterion = torch.nn.BCEWithLogitsLoss()\n\n        # Train model\n        self.logger.info(\"Training final model...\")\n        history = self.train_model(\n            model, train_loader, test_loader,\n            optimizer, criterion, verbose=True\n        )\n\n        # Evaluate on test set\n        pred_test, prob_test, metrics_test = self.evaluate_model(\n            model, X_test_tensor, y_test_tensor\n        )\n\n        cm_test = confusion_matrix(y_test_tensor.cpu().numpy(), pred_test)\n\n        # Classification report\n        y_test_labels = ['D' if x == 0 else 'E' for x in y_test_tensor.cpu().numpy()]\n        pred_test_labels = ['D' if x == 0 else 'E' for x in pred_test]\n        class_report = classification_report(y_test_labels, pred_test_labels, target_names=['D', 'E'])\n\n        self.logger.info(\"\")\n        self.logger.info(\"Final Model Test Results:\")\n        self.logger.info(\"-\" * 80)\n        self.logger.info(f\"Accuracy:     {metrics_test['Accuracy']:.4f}\")\n        self.logger.info(f\"Precision:    {metrics_test['Precision']:.4f}\")\n        self.logger.info(f\"Recall:       {metrics_test['Recall']:.4f}\")\n        self.logger.info(f\"Specificity:  {metrics_test['Specificity']:.4f}\")\n        self.logger.info(f\"F1-Score:     {metrics_test['F1-Score']:.4f}\")\n        self.logger.info(f\"MCC:          {metrics_test['MCC']:.4f}\")\n        self.logger.info(\"-\" * 80)\n        self.logger.info(f\"\\nClassification Report:\\n{class_report}\")\n        self.logger.info(f\"Confusion Matrix:\\n{cm_test}\")\n\n        # Save model and results\n        best_model_dir = os.path.join(self.config.OUTPUT_PATH, 'best_model')\n        Path(best_model_dir).mkdir(parents=True, exist_ok=True)\n\n        best_model_path = os.path.join(best_model_dir, 'best_model.pth')\n        best_scaler_path = os.path.join(best_model_dir, 'best_scaler.pkl')\n        best_params_path = os.path.join(best_model_dir, 'best_hyperparameters.pkl')\n\n        torch.save(model.state_dict(), best_model_path)\n        with open(best_scaler_path, 'wb') as f:\n            pickle.dump(scaler, f)\n        with open(best_params_path, 'wb') as f:\n            pickle.dump(best_params, f)\n\n        self.logger.info(f\"\\nBest model saved to: {best_model_path}\")\n        self.logger.info(f\"Best scaler saved to: {best_scaler_path}\")\n        self.logger.info(f\"Best hyperparameters saved to: {best_params_path}\")\n\n        # Save predictions\n        predictions_df = pd.DataFrame({\n            'Actual': test_labels,\n            'Predicted': pred_test,\n            'Probability': prob_test\n        })\n        predictions_file = os.path.join(best_model_dir, 'test_predictions.xlsx')\n        predictions_df.to_excel(predictions_file, index=False)\n        self.logger.info(f\"Predictions saved to: {predictions_file}\")\n\n        # Save confusion matrix\n        cm_file = os.path.join(best_model_dir, 'confusion_matrix.png')\n        save_confusion_matrix(\n            cm_test, cm_file, self.config.CLASS_NAMES,\n            'Best Model - Test Set Confusion Matrix'\n        )\n        self.logger.info(f\"Confusion matrix saved to: {cm_file}\")\n\n        # Save detailed metrics\n        metrics_file = os.path.join(best_model_dir, 'final_metrics.txt')\n        with open(metrics_file, 'w', encoding='utf-8') as f:\n            f.write(\"=\" * 80 + \"\\n\")\n            f.write(\"FINAL MODEL METRICS\\n\")\n            f.write(\"=\" * 80 + \"\\n\\n\")\n            f.write(\"Optimal Hyperparameters:\\n\")\n            f.write(\"-\" * 80 + \"\\n\")\n            for param_name, param_value in best_params.items():\n                f.write(f\"  {param_name:22s}: {param_value}\\n\")\n            f.write(\"-\" * 80 + \"\\n\\n\")\n            f.write(\"Test Set Metrics:\\n\")\n            f.write(\"-\" * 80 + \"\\n\")\n            f.write(f\"Accuracy:     {metrics_test['Accuracy']:.16f}\\n\")\n            f.write(f\"Precision:    {metrics_test['Precision']:.16f}\\n\")\n            f.write(f\"Recall:       {metrics_test['Recall']:.16f}\\n\")\n            f.write(f\"Specificity:  {metrics_test['Specificity']:.16f}\\n\")\n            f.write(f\"F1-Score:     {metrics_test['F1-Score']:.16f}\\n\")\n            f.write(f\"MCC:          {metrics_test['MCC']:.16f}\\n\")\n            f.write(\"-\" * 80 + \"\\n\\n\")\n            f.write(f\"Classification Report:\\n{class_report}\\n\")\n            f.write(f\"\\nConfusion Matrix:\\n{cm_test}\\n\")\n\n        self.logger.info(f\"Detailed metrics saved to: {metrics_file}\")\n\n        return {\n            'model': model,\n            'scaler': scaler,\n            'predictions': pred_test,\n            'probabilities': prob_test,\n            'confusion_matrix': cm_test,\n            'metrics': metrics_test,\n            'classification_report': class_report,\n            'history': history\n        }\n\n\nclass ResultsManager:\n    \"\"\"Generate comprehensive reports and visualizations.\"\"\"\n\n    def __init__(self, config: Config, logger: logging.Logger):\n        self.config = config\n        self.logger = logger\n\n    def save_results(\n        self,\n        grid_search_results: Dict,\n        training_results: Dict\n    ) -> None:\n        \"\"\"Save all results and generate visualizations.\"\"\"\n        print_section_header(\"SAVING RESULTS AND GENERATING REPORTS\", self.logger)\n\n        output_excel = os.path.join(self.config.OUTPUT_PATH, 'kan_comprehensive_results.xlsx')\n\n        with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:\n            # Grid search results\n            grid_df = pd.DataFrame(grid_search_results['grid_results'])\n            grid_df = grid_df.sort_values('accuracy', ascending=False)\n            grid_df.to_excel(writer, sheet_name='Grid_Search_Results', index=False)\n\n            # Best parameters\n            best_params_df = pd.DataFrame([grid_search_results['best_params']])\n            best_params_df.to_excel(writer, sheet_name='Optimal_Parameters', index=False)\n\n            # Final metrics\n            final_metrics_df = pd.DataFrame([training_results['metrics']])\n            final_metrics_df.to_excel(writer, sheet_name='Final_Metrics', index=False)\n\n        self.logger.info(f\"Comprehensive results saved to: {output_excel}\")\n        self.logger.info(\"All visualizations generated successfully\")\n\n\ndef main():\n    \"\"\"Main execution pipeline for KAN training.\"\"\"\n\n    logger = setup_logging(Config.OUTPUT_PATH)\n    Config.display_config(logger)\n\n    try:\n        # Load data\n        data_manager = DataManager(Config, logger)\n        train_features, train_labels, test_features, test_labels = data_manager.load_data()\n\n        # Initialize trainer\n        trainer = KANTrainer(Config, logger)\n\n        # Grid search\n        grid_search_results = trainer.grid_search(\n            train_features, train_labels,\n            test_features, test_labels\n        )\n\n        # Save grid search results\n        grid_file = os.path.join(Config.OUTPUT_PATH, 'grid_search_results.xlsx')\n        grid_df = pd.DataFrame(grid_search_results['grid_results'])\n        grid_df = grid_df.sort_values('accuracy', ascending=False)\n        grid_df.to_excel(grid_file, index=False)\n        logger.info(f\"\\nGrid search results saved to: {grid_file}\")\n\n        # Train final model\n        training_results = trainer.train_final_model(\n            train_features, train_labels,\n            test_features, test_labels,\n            grid_search_results['best_params'],\n            grid_search_results['scaler']\n        )\n\n        # Save comprehensive results\n        results_manager = ResultsManager(Config, logger)\n        results_manager.save_results(grid_search_results, training_results)\n\n        print_section_header(\"TRAINING COMPLETED SUCCESSFULLY\", logger)\n        logger.info(f\"\\nAll outputs saved to: {Config.OUTPUT_PATH}\")\n        logger.info(f\"Best test accuracy: {grid_search_results['best_accuracy']:.4f}\")\n\n    except Exception as e:\n        logger.error(f\"Training failed: {str(e)}\", exc_info=True)\n        raise\n\n    finally:\n        logger.info(\"\")\n        logger.info(\"=\" * 80)\n        logger.info(f\"Session ended: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        logger.info(\"=\" * 80)\n\n\nif __name__ == \"__main__\":\n    main()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}