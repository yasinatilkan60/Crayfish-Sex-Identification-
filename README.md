# Crayfish Sex Classification

## Enhancing Crayfish Sex Identification with Kolmogorov-Arnold Networks and Stacked Autoencoders

This repository contains the source code and experimental results for the research paper:

> **"Enhancing Crayfish Sex Identification with Kolmogorov-Arnold Networks and Stacked Autoencoders"**
>
> Atilkan, Y., Kirik, B., Acikbas, E.T., Ekinci, F., Acici, K., Asuroglu, T., Benzer, R., Guzel, M.S., & Benzer, S. (2025)

---

## Abstract

Crayfish play an important role in freshwater ecosystems, and sex classification is crucial for analyzing their demographic structures. This study performed binary classification using traditional machine learning and deep learning models on tabular and image datasets with an imbalanced class distribution. For tabular classification, features related to crayfish weight and size were used. Missing values were handled using different methods to create various datasets. **Kolmogorov-Arnold Networks demonstrated the best performance across all metrics, achieving accuracy rates between 95% and 100%**. Image data were generated by combining at least five photographs of each crayfish. Autoencoders were employed to extract meaningful features. In experiments conducted on these extracted features, support vector machines achieved 84% accuracy, and multilayer perceptrons achieved 82% accuracy, outperforming other models. To enhance performance, a novel architecture based on stacked autoencoders was proposed. While some models experienced performance declines, **Kolmogorov-Arnold Networks showed an average improvement of 3.5% across all metrics**, maintaining the highest accuracy. To statistically evaluate performance differences, McNemar's and Wilcoxon tests were applied. The results confirmed significant differences between Kolmogorov-Arnold Networks, support vector machines, multilayer perceptrons, and naive Bayes.

**Keywords**: crayfish, sex identification, deep learning, machine learning, Kolmogorov-Arnold Networks, stacked autoencoders

---

## Authors

| Author | Affiliation | Email |
|--------|-------------|-------|
| Yasin Atilkan | ¹Department of AI and Data Engineering, Ankara University | yatilkan@ankara.edu.tr |
| Berk Kirik | ²Department of Biomedical Engineering, Ankara University | berk.kirik@outlook.com |
| Eren Tuna Acikbas | ³Department of Petroleum and Natural Gas Engineering, METU | acikbas.tuna@metu.edu.tr |
| Fatih Ekinci | ⁴Institute of Artificial Intelligence, Ankara University | fatihekinci@ankara.edu.tr |
| Koray Acici | ¹'⁴Ankara University | kacici@ankara.edu.tr |
| **Tunc Asuroglu*** | ⁵Faculty of Medicine and Health Technology, Tampere University; ⁶VTT Technical Research Centre of Finland | tunc.asuroglu@tuni.fi |
| Recep Benzer | ⁷Department of Management Information System, Ankara Medipol University | recep.benzer@ankaramedipol.edu.tr |
| Mehmet Serdar Guzel | ⁴'⁸Department of Computer Engineering, Ankara University | mguzel@ankara.edu.tr |
| Semra Benzer | ⁹Department of Science Education, Gazi University | sbenzer@gazi.edu.tr |

*\* Corresponding Author*

---

## Dataset

### Species Information
- **Species**: *Pontastacus leptodactylus* Eschscholtz, 1823 (Narrow-clawed crayfish)
- **Common Name**: Turkish crayfish, Danube crayfish
- **Ecological Significance**: Biological indicator for freshwater ecosystem health

### Data Collection
- **Source Locations**: Eğirdir Lake, Beyşehir Lake, Hirfanlı Lake (Turkey)
- **Collection Period**: 2017-2018 fishing seasons
- **Collection Method**: Obtained from local fishermen

### Tabular Dataset
| Property | Value |
|----------|-------|
| Total Specimens | 112 |
| Female (D) | 62 (55.4%) |
| Male (E) | 50 (44.6%) |
| Class Ratio | 0.806:1 |

### Morphometric Features
| Feature | Description |
|---------|-------------|
| W | Weight |
| CL (KB) | Carapace Length |
| Cw (KE) | Carapace Width |
| AL (AB) | Abdomen Length |
| Aw (AE) | Abdomen Width |
| ChlL (K_Sag, K_Sol) | Cheliped Length (Right, Left) |
| Chw (U_Sag, U_Sol) | Cheliped Width (Right, Left) |
| ChL (KE_Sag, KE_Sol) | Cheliped Height (Right, Left) |

### Image Dataset
| Property | Value |
|----------|-------|
| Total Images | 1,277 |
| Female Images | 717 (56.1%) |
| Male Images | 560 (43.9%) |
| Original Resolution | 4608 × 3456 pixels |
| Processed Resolution | 28 × 28 pixels (grayscale) |
| Train/Test Split | 70% / 30% |

> **Note**: Image dataset is not publicly available. Only tabular data is provided in the Zenodo repository.

### Data Availability
The **tabular dataset** is publicly available on Zenodo:

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17516963.svg)](https://doi.org/10.5281/zenodo.17516963)

---

## Methodology

### General Framework

The study follows three experimental frameworks:

**Framework 1: Tabular Data Classification**
```
Tabular Data (112 samples, 11 features)
         │
         ▼
┌─────────────────────────┐
│   Missing Value         │
│   Imputation            │
│   (Mean/Median/Mode/KNN)│
└─────────────────────────┘
         │
         ▼
┌─────────────────────────┐
│   Outlier Correction    │
│   (IQR Method)          │
└─────────────────────────┘
         │
         ▼
┌─────────────────────────┐
│   Standardization /     │
│   Min-Max Normalization │
└─────────────────────────┘
         │
         ▼
┌─────────────────────────┐
│   Hyperparameter        │
│   Optimization          │
│   (10-Fold CV)          │
└─────────────────────────┘
         │
         ▼
┌─────────────────────────┐
│   Classification        │
│   (SVM, NB, KNN,        │
│    MLP, RF, KAN)        │
└─────────────────────────┘
         │
         ▼
┌─────────────────────────┐
│   Evaluation            │
│   (10-Fold CV)          │
└─────────────────────────┘
```

**Framework 2 & 3: Image-Based Classification**
```
Image Data (1,277 images, 4608×3456 pixels)
         │
         ▼
┌─────────────────────────┐
│   Preprocessing         │
│   - Grayscale conversion│
│   - Resize to 28×28     │
│   - Normalization       │
└─────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│         Feature Extraction              │
├─────────────────┬───────────────────────┤
│   Autoencoder   │   Stacked Autoencoder │
│   (12,800 feat.)│   (18,432 feat.)      │
└─────────────────┴───────────────────────┘
         │
         ▼
┌─────────────────────────┐
│   Train/Test Split      │
│   (70% / 30%)           │
└─────────────────────────┘
         │
         ▼
┌─────────────────────────┐
│   Hyperparameter        │
│   Optimization          │
│   (10-Fold CV on Train) │
└─────────────────────────┘
         │
         ▼
┌─────────────────────────┐
│   Classification        │
│   (SVM, NB, KNN,        │
│    MLP, RF, KAN)        │
└─────────────────────────┘
         │
         ▼
┌─────────────────────────┐
│   Evaluation            │
│   (Test Set)            │
└─────────────────────────┘
```

### Data Preprocessing

#### Missing Value Imputation
| Method | Description |
|--------|-------------|
| Mean | Replace missing values with feature mean |
| Median | Replace missing values with feature median |
| Mode | Replace missing values with feature mode |
| KNN (k=5) | Estimate missing values using 5 nearest neighbors |

#### Outlier Correction (IQR Method)
```
Lower Boundary = Q1 - 3 × IQR
Upper Boundary = Q3 + 3 × IQR
```

#### Normalization
- **StandardScaler**: Zero mean, unit variance
- **Min-Max Normalization**: Scale to [0, 1] range

### Feature Extraction Architectures

#### Convolutional Autoencoder
- **Encoder**: 2 convolutional layers
- **Latent Space**: Compressed representation
- **Decoder**: 2 transposed convolutional layers
- **Output Features**: 12,800

#### Stacked Convolutional Autoencoder
- **Architecture**: Two autoencoders stacked sequentially
- **First Autoencoder**: Initial feature extraction
- **Second Autoencoder**: Higher-level feature extraction
- **Output Features**: 18,432

### Classification Models

| Model | Description | Key Parameters |
|-------|-------------|----------------|
| **SVM** | Support Vector Machine | RBF kernel |
| **NB** | Gaussian Naive Bayes | Default parameters |
| **KNN** | K-Nearest Neighbors | Euclidean distance |
| **MLP** | Multilayer Perceptron | Adam optimizer |
| **RF** | Random Forest | CART algorithm |
| **KAN** | Kolmogorov-Arnold Network | B-spline transformations |

### Kolmogorov-Arnold Networks (KAN)

KAN is based on the Kolmogorov-Arnold representation theorem, decomposing complex functions into univariate components using B-spline transformations.

**Architecture Configuration (Search Space):**
| Parameter | Search Space |
|-----------|--------------|
| Hidden Layers | [11,256,128,64,32,1], [11,128,64,32,1], [11,32,1], [11,128,64,32,16,1] |
| Grid Size | [4, 5, 6, 7, 8] |
| Spline Order | [3, 4, 5] |
| Scale Base | [1.0, 2.0, 3.0] |
| Scale Spline | [1.0, 2.0] |
| Batch Size | [32, 64, 128] |
| Optimizer | Adam, SGD |
| Learning Rate | [0.0001, 0.001, 0.005, 0.0005] |

**Training Configuration:**
- Early Stopping: Patience = 10 epochs
- Maximum Epochs: 50
- Loss Function: BCEWithLogitsLoss

### Validation Strategy

| Dataset Type | Validation Method |
|--------------|-------------------|
| Tabular Data | 10-Fold Cross-Validation |
| Image Features | 70% Train / 30% Test Split |

---

## Project Structure

```
Crayfish-Sex-Identification-/
│
├── README.md
├── .gitignore
│
├── crayfish_gender_classification_tabular/
│   │
│   ├── 1.mean/                          # Mean imputation dataset
│   │   └── classification/
│   │       ├── ann/                     # Artificial Neural Network
│   │       ├── kan/                     # Kolmogorov-Arnold Network
│   │       ├── knn/                     # K-Nearest Neighbors
│   │       ├── naive_bayes/             # Gaussian Naive Bayes
│   │       ├── rf/                      # Random Forest
│   │       └── svm/                     # Support Vector Machine
│   │
│   ├── 2.median/                        # Median imputation dataset
│   │   └── classification/
│   │       ├── ann/
│   │       ├── kan/
│   │       ├── knn/
│   │       ├── naive_bayes/
│   │       ├── rf/
│   │       └── svm/
│   │
│   ├── 3.mod/                           # Mode imputation dataset
│   │   └── classification/
│   │       ├── ann/
│   │       ├── kan/
│   │       ├── knn/
│   │       ├── naive_bayes/
│   │       ├── rf/
│   │       └── svm/
│   │
│   ├── 4.knn/                           # KNN imputation dataset
│   │   └── classification/
│   │       ├── ann/
│   │       ├── kan/
│   │       ├── knn/
│   │       ├── naive_bayes/
│   │       ├── rf/
│   │       └── svm/
│   │
│   └── 5.min_maks_normal/               # Min-Max normalized dataset
│       └── wostand/
│           ├── ann22/
│           ├── kan/
│           ├── knn22/
│           ├── nb22/
│           ├── rf22/
│           └── svm22/
│
├── ae/                                   # Autoencoder experiments
│   ├── Autoencoder.ipynb                # Autoencoder training
│   ├── ann/
│   ├── kan/
│   ├── knn/
│   ├── nb/
│   ├── rf/
│   └── svm/
│
└── sae2/                                 # Stacked Autoencoder experiments
    ├── StackedAutoencoder.ipynb         # Stacked AE training
    ├── ann/
    ├── kan/
    ├── knn/
    ├── nb/
    ├── rf/
    └── svm/
```

---

## Experimental Results

### Evaluation Metrics

| Metric | Formula |
|--------|---------|
| Accuracy | (TP + TN) / (TP + TN + FP + FN) |
| Precision | TP / (TP + FP) |
| Recall (Sensitivity) | TP / (TP + FN) |
| Specificity | TN / (TN + FP) |
| F1-Score | 2 × (Precision × Recall) / (Precision + Recall) |
| MCC | (TP×TN - FP×FN) / √((TP+FP)(TP+FN)(TN+FP)(TN+FN)) |

### Tabular Data Results

#### Table 3: Mean Imputation + Standardization

| Model | Accuracy | Recall | Specificity | Precision | F1-Score | MCC |
|-------|----------|--------|-------------|-----------|----------|-----|
| **KAN** | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** |
| SVM | 0.821 | 0.780 | 0.855 | 0.813 | 0.796 | 0.638 |
| MLP | 0.804 | 0.800 | 0.807 | 0.769 | 0.784 | 0.605 |
| KNN | 0.705 | 0.660 | 0.742 | 0.674 | 0.667 | 0.403 |
| RF | 0.696 | 0.580 | 0.790 | 0.691 | 0.630 | 0.380 |
| NB | 0.571 | 0.360 | 0.742 | 0.529 | 0.429 | 0.110 |

#### Table 4: Median Imputation + Standardization

| Model | Accuracy | Recall | Specificity | Precision | F1-Score | MCC |
|-------|----------|--------|-------------|-----------|----------|-----|
| **KAN** | **0.982** | **0.980** | **0.982** | **0.980** | **0.980** | **0.964** |
| SVM | 0.813 | 0.760 | 0.855 | 0.809 | 0.784 | 0.619 |
| MLP | 0.795 | 0.720 | 0.855 | 0.800 | 0.758 | 0.583 |
| KNN | 0.741 | 0.700 | 0.774 | 0.714 | 0.707 | 0.475 |
| RF | 0.688 | 0.580 | 0.774 | 0.674 | 0.624 | 0.362 |
| NB | 0.589 | 0.380 | 0.758 | 0.559 | 0.452 | 0.149 |

#### Table 5: Mode Imputation + Standardization

| Model | Accuracy | Recall | Specificity | Precision | F1-Score | MCC |
|-------|----------|--------|-------------|-----------|----------|-----|
| **KAN** | **0.991** | **1.000** | **0.984** | **0.980** | **0.990** | **0.982** |
| SVM | 0.830 | 0.820 | 0.839 | 0.804 | 0.812 | 0.658 |
| MLP | 0.786 | 0.720 | 0.839 | 0.783 | 0.750 | 0.565 |
| KNN | 0.714 | 0.640 | 0.774 | 0.696 | 0.667 | 0.419 |
| RF | 0.688 | 0.580 | 0.774 | 0.674 | 0.624 | 0.362 |
| NB | 0.616 | 0.400 | 0.790 | 0.606 | 0.482 | 0.208 |

#### Table 6: KNN Imputation + Standardization

| Model | Accuracy | Recall | Specificity | Precision | F1-Score | MCC |
|-------|----------|--------|-------------|-----------|----------|-----|
| **KAN** | **0.982** | **1.000** | **0.968** | **0.962** | **0.980** | **0.965** |
| SVM | 0.830 | 0.820 | 0.839 | 0.804 | 0.812 | 0.658 |
| MLP | 0.804 | 0.800 | 0.807 | 0.769 | 0.784 | 0.605 |
| KNN | 0.732 | 0.660 | 0.790 | 0.717 | 0.688 | 0.455 |
| RF | 0.661 | 0.560 | 0.742 | 0.636 | 0.596 | 0.307 |
| NB | 0.571 | 0.300 | 0.790 | 0.536 | 0.385 | 0.104 |

#### Table 7: Mode Imputation + Min-Max Normalization

| Model | Accuracy | Recall | Specificity | Precision | F1-Score | MCC |
|-------|----------|--------|-------------|-----------|----------|-----|
| **KAN** | **0.973** | **0.960** | **0.984** | **0.980** | **0.970** | **0.946** |
| SVM | 0.813 | 0.800 | 0.823 | 0.784 | 0.792 | 0.622 |
| MLP | 0.813 | 0.800 | 0.823 | 0.784 | 0.792 | 0.622 |
| KNN | 0.750 | 0.660 | 0.823 | 0.750 | 0.702 | 0.491 |
| RF | 0.661 | 0.540 | 0.758 | 0.643 | 0.587 | 0.306 |
| NB | 0.616 | 0.400 | 0.790 | 0.606 | 0.482 | 0.208 |

### Image Feature Results

#### Table 8: Autoencoder Features (12,800 features)

| Model | Accuracy | Recall | Specificity | Precision | F1-Score | MCC |
|-------|----------|--------|-------------|-----------|----------|-----|
| **SVM** | **0.840** | **0.855** | **0.829** | **0.793** | **0.823** | **0.680** |
| MLP | 0.819 | 0.813 | 0.824 | 0.780 | 0.797 | 0.635 |
| KNN | 0.801 | 0.759 | 0.833 | 0.778 | 0.768 | 0.594 |
| KAN | 0.780 | 0.795 | 0.769 | 0.725 | 0.759 | 0.560 |
| RF | 0.778 | 0.705 | 0.833 | 0.765 | 0.734 | 0.544 |
| NB | 0.631 | 0.705 | 0.574 | 0.560 | 0.624 | 0.278 |

#### Table 9: Stacked Autoencoder Features (18,432 features)

| Model | Accuracy | Recall | Specificity | Precision | F1-Score | MCC |
|-------|----------|--------|-------------|-----------|----------|-----|
| **KAN** | **0.819** | **0.807** | **0.829** | **0.784** | **0.795** | **0.634** |
| SVM | 0.806 | 0.807 | 0.806 | 0.761 | 0.784 | 0.609 |
| KNN | 0.796 | 0.759 | 0.824 | 0.768 | 0.764 | 0.584 |
| MLP | 0.783 | 0.771 | 0.792 | 0.740 | 0.755 | 0.560 |
| RF | 0.780 | 0.711 | 0.833 | 0.766 | 0.738 | 0.550 |
| NB | 0.636 | 0.681 | 0.602 | 0.568 | 0.619 | 0.280 |

### Key Findings

1. **KAN achieves best performance on tabular data** (95-100% accuracy)
2. **Mode imputation yields highest total accuracy** across all models
3. **SVM performs best on autoencoder features** (84% accuracy)
4. **KAN improves 3.5% with stacked autoencoders**, achieving best performance
5. **Statistical tests confirm** significant differences between KAN, SVM, MLP, and NB

---

## Optimal Hyperparameters

### Appendix B: Selected Hyperparameters for Tabular Data (Framework 1)

#### KAN Best Hyperparameters

| Dataset | layers_hidden | grid_size | spline_order | scale_base | scale_spline | batch_size | optimizer | learning_rate |
|---------|---------------|-----------|--------------|------------|--------------|------------|-----------|---------------|
| Mean | [11,128,64,32,1] | 7 | 5 | 1.0 | 1.0 | 64 | Adam | 0.0005 |
| Median | [11,128,64,32,1] | 7 | 5 | 1.0 | 1.0 | 32 | Adam | 0.0001 |
| Mode | [11,128,64,32,1] | 7 | 5 | 1.0 | 1.0 | 64 | Adam | 0.005 |
| KNN | [11,128,64,32,1] | 7 | 5 | 1.0 | 1.0 | 64 | Adam | 0.0005 |
| Min-Max | [11,128,64,32,1] | 8 | 4 | 1.0 | 1.0 | 64 | Adam | 0.005 |

#### Other Models Best Hyperparameters

| Dataset | MLP (hidden_layer_sizes, activation, lr, solver) | KNN (n_neighbors) | RF (max_depth) | SVM (C, gamma) |
|---------|--------------------------------------------------|-------------------|----------------|----------------|
| Mean | (64,64), relu, 0.01, sgd | 7 | 10 | 1e9, 1e-6 |
| Median | (32,), relu, 0.001, adam | 7 | 9 | 10, 0.01 |
| Mode | (128,128), tanh, 0.01, adam | 7 | 6 | 1000, 0.001 |
| KNN | (64,64,64), relu, 0.01, sgd | 3 | 9 | 1000, 0.001 |
| Min-Max | (64,64), relu, 0.001, adam | 5 | 6 | 10000, 0.01 |

### Appendix D: Selected Hyperparameters for Image Features (Framework 2 & 3)

#### KAN Best Hyperparameters for Autoencoder Features

| Parameter | Autoencoder (12,800 features) | Stacked Autoencoder (18,432 features) |
|-----------|-------------------------------|---------------------------------------|
| layers_hidden | [12800,128,64,32,1] | [18432,128,64,32,1] |
| grid_size | 7 | 7 |
| spline_order | 5 | 5 |
| scale_base | 1.0 | 1.0 |
| scale_spline | 1.0 | 1.0 |
| batch_size | 64 | 64 |
| optimizer | Adam | Adam |
| learning_rate | 0.0005 | 0.0005 |

#### Other Models Best Hyperparameters for Image Features

| Model | Autoencoder Features | Stacked Autoencoder Features |
|-------|---------------------|------------------------------|
| MLP | hidden=(128,128), activation=relu, lr=0.001, solver=sgd | hidden=(128,128,128), activation=relu, lr=0.001, solver=sgd |
| KNN | n_neighbors=1 | n_neighbors=1 |
| RF | max_depth=9 | max_depth=10 |
| SVM | C=10, gamma=0.0001 | C=10, gamma=0.0001 |

---

## Statistical Analysis

### Wilcoxon Signed-Rank Test Results

Statistical significance evaluated at α = 0.05. Arrows indicate the better-performing model.

| Comparison | Min-Max Tabular | Autoencoder | Stacked AE |
|------------|-----------------|-------------|------------|
| KAN vs NB | **← (p < 0.05)** | **← (p < 0.05)** | **← (p < 0.05)** |
| KAN vs SVM | - | - | - |
| KAN vs MLP | - | - | - |
| KAN vs RF | - | **← (p < 0.05)** | **← (p < 0.05)** |
| KAN vs KNN | - | **← (p < 0.05)** | - |
| SVM vs NB | **← (p < 0.05)** | **← (p < 0.05)** | **← (p < 0.05)** |
| MLP vs NB | **← (p < 0.05)** | **← (p < 0.05)** | **← (p < 0.05)** |

### McNemar's Test Results

Results confirm similar findings to Wilcoxon tests, validating statistical significance of performance differences.

---

## Installation

### Requirements

- Python 3.9+
- PyTorch 2.0+
- scikit-learn 1.5.0+
- CUDA (optional, for GPU acceleration)

### Setup

```bash
# Clone the repository
git clone https://github.com/yasinatilkan60/Crayfish-Sex-Identification-.git
cd Crayfish-Sex-Identification-

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Linux/macOS:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

# Install dependencies
pip install torch>=2.0.0
pip install scikit-learn>=1.5.0
pip install pandas>=2.0.0
pip install numpy>=1.24.0
pip install matplotlib>=3.7.0
pip install seaborn>=0.12.0
pip install openpyxl>=3.1.0
pip install tqdm>=4.65.0

# For KAN implementation
pip install pykan
# or clone efficient-kan/fastkan repositories
```

### Google Colab Setup

```python
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Add KAN package paths
import sys
sys.path.append('/content/drive/MyDrive/packages/efficient_kan')
sys.path.append('/content/drive/MyDrive/packages/fastkan')

# Import KAN
from kan import KAN
```

---

## Usage

### Running Tabular Classification

1. Navigate to the desired imputation method directory:
```bash
cd crayfish_gender_classification_tabular/1.mean/classification/kan/
```

2. Open and run the Jupyter notebook:
```bash
jupyter notebook KAN_Classification.ipynb
```

3. Update data paths in the notebook configuration:
```python
class Config:
    DATA_PATH = '/path/to/your/data.xlsx'
    OUTPUT_PATH = '/path/to/results/'
```

### Running Autoencoder Feature Extraction

1. Train the autoencoder:
```bash
cd ae/
jupyter notebook Autoencoder.ipynb
```

2. Run classification on extracted features:
```bash
cd ae/kan/
jupyter notebook KAN_Classification.ipynb
```

### Running Stacked Autoencoder Experiments

1. Train the stacked autoencoder:
```bash
cd sae2/
jupyter notebook StackedAutoencoder.ipynb
```

2. Run classification:
```bash
cd sae2/kan/
jupyter notebook KAN_Classification.ipynb
```

---

## Citation

If you use this code or dataset in your research, please cite:

```bibtex
@article{atilkan2025crayfish,
  title={Enhancing Crayfish Sex Identification with Kolmogorov-Arnold Networks and Stacked Autoencoders},
  author={Atilkan, Yasin and Kirik, Berk and Acikbas, Eren Tuna and Ekinci, Fatih and Acici, Koray and Asuroglu, Tunc and Benzer, Recep and Guzel, Mehmet Serdar and Benzer, Semra},
  journal={},
  year={2025},
  doi={}
}
```

### Dataset Citation

```bibtex
@dataset{benzer2025crayfish,
  author={Benzer, Semra},
  title={Crayfish Sex Classification Dataset},
  year={2025},
  publisher={Zenodo},
  doi={10.5281/zenodo.17516963},
  url={https://doi.org/10.5281/zenodo.17516963}
}
```

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

We thank the local fishermen from Eğirdir Lake, Beyşehir Lake, and Hirfanlı Lake for their assistance in specimen collection during the 2017-2018 fishing seasons.

---

## KAN Implementation Contact

For KAN class implementation details, please contact:

- **Yasin Atilkan** - yatilkan@ankara.edu.tr
- **Berk Kirik** - berk.kirik@outlook.com
- **Eren Tuna Acikbas** - acikbas.tuna@metu.edu.tr

---

## References

1. Liu, Z. et al. (2024). KAN: Kolmogorov-Arnold Networks. *arXiv preprint arXiv:2404.19756*.
2. Masci, J. et al. (2011). Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction. *ICANN 2011*.
3. Crandall, K.A. & De Grave, S. (2017). An updated classification of the freshwater crayfishes. *Journal of Crustacean Biology*, 37, 615-653.
